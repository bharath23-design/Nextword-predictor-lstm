{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3XQxawqFX_8",
        "outputId": "0adf139e-4231-4b5c-ee41-f550aaa1b25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6315hKK7Fz1x"
      },
      "source": [
        "# load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49OerDBZFiGB"
      },
      "outputs": [],
      "source": [
        "questions='''\n",
        "1. What is Deep Learning?\n",
        "Deep Learning involves taking large volumes of structured or unstructured data and\n",
        "using complex algorithms to train neural networks. It performs complex operations\n",
        "to extract hidden patterns and features (for instance, distinguishing the image of a\n",
        "cat from that of a dog).\n",
        "2. What is a Neural Network?\n",
        "Neural Networks replicate the way humans learn, inspired by how the neurons in our\n",
        "brains fire, only much simpler.\n",
        "The most common Neural Networks consist of three network layers:\n",
        "1. An input layer\n",
        "2. A hidden layer (this is the most important layer where feature extraction\n",
        "takes place, and adjustments are made to train faster and function better)\n",
        "3. An output layer\n",
        "Each sheet contains neurons called “nodes,” performing various operations. Neural\n",
        "Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.\n",
        "3. What Is a Multi-layer Perceptron(MLP)?\n",
        "As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer.\n",
        "It has the same structure as a single layer perceptron with one or more hidden\n",
        "layers. A single layer perceptron can classify only linear separable classes with\n",
        "binary output (0,1), but MLP can classify nonlinear classes.\n",
        "Except for the input layer, each node in the other layers uses a nonlinear activation\n",
        "function. This means the input layers, the data coming in, and the activation function\n",
        "is based upon all nodes and weights being added together, producing the output.\n",
        "MLP uses a supervised learning method called “backpropagation.” In\n",
        "backpropagation, the neural network calculates the error with the help of cost\n",
        "function. It propagates this error backward from where it came (adjusts the weights\n",
        "to train the model more accurately).\n",
        "4. What Is Data Normalization, and Why Do We Need It?\n",
        "The process of standardizing and reforming data is called “Data Normalization.” It’s\n",
        "a pre-processing step to eliminate data redundancy. Often, data comes in, and you\n",
        "get the same information in different formats. In these cases, you should rescale\n",
        "values to fit into a particular range, achieving better convergence.\n",
        "5. What is the Boltzmann Machine?\n",
        "One of the most basic Deep Learning models is a Boltzmann Machine, resembling a\n",
        "simplified version of the Multi-Layer Perceptron. This model features a visible input\n",
        "layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions\n",
        "as to whether a neuron should be on or off. Nodes are connected across layers, but\n",
        "no two nodes of the same layer are connected.\n",
        "6. What Is the Role of Activation Functions in a Neural Network?\n",
        "At the most basic level, an activation function decides whether a neuron should be\n",
        "fired or not. It accepts the weighted sum of the inputs and bias as input to any\n",
        "activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples\n",
        "of activation functions.\n",
        "7. What Is the Cost Function?\n",
        "Also referred to as “loss” or “error,” cost function is a measure to evaluate how good\n",
        "your model’s performance is. It’s used to compute the error of the output layer\n",
        "during backpropagation. We push that error backward through the neural network\n",
        "and use that during the different training functions.\n",
        "8. What Is Gradient Descent?\n",
        "Gradient Descent is an optimal algorithm to minimize the cost function or to\n",
        "minimize an error. The aim is to find the local-global minima of a function. This\n",
        "determines the direction the model should take to reduce the error.\n",
        "9. What Do You Understand by Backpropagation?\n",
        "Backpropagation is a technique to improve the performance of the network. It\n",
        "backpropagates the error and updates the weights to reduce the error.\n",
        "10. What Is the Difference Between a Feedforward Neural Network and\n",
        "Recurrent Neural Network?\n",
        "A Feedforward Neural Network signals travel in one direction from input to output.\n",
        "There are no feedback loops; the network considers only the current input. It cannot\n",
        "memorize previous inputs (e.g., CNN).\n",
        "11. What Are the Applications of a Recurrent Neural Network (RNN)?\n",
        "The RNN can be used for sentiment analysis, text mining, and image captioning.\n",
        "Recurrent Neural Networks can also address time series problems such as\n",
        "predicting the prices of stocks in a month or quarter.\n",
        "12. What Are the Softmax and ReLU Functions?\n",
        "Softmax is an activation function that generates the output between zero and one. It\n",
        "divides each output, such that the total sum of the outputs is equal to one. Softmax\n",
        "is often used for output layers.\n",
        "ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an\n",
        "output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers.\n",
        "13. What Are Hyperparameters?\n",
        "With neural networks, you’re usually working with hyperparameters once the data is\n",
        "formatted correctly. A hyperparameter is a parameter whose value is set before the\n",
        "learning process begins. It determines how a network is trained and the structure of\n",
        "the network (such as the number of hidden units, the learning rate, epochs, etc.).\n",
        "14. What Will Happen If the Learning Rate Is Set Too Low or Too High?\n",
        "When your learning rate is too low, training of the model will progress very slowly as\n",
        "we are making minimal updates to the weights. It will take many updates before\n",
        "reaching the minimum point.\n",
        "If the learning rate is set too high, this causes undesirable divergent behavior to the\n",
        "loss function due to drastic updates in weights. It may fail to converge (model can\n",
        "give a good output) or even diverge (data is too chaotic for the network to train).\n",
        "15. What Is Dropout and Batch Normalization?\n",
        "Dropout is a technique of dropping out hidden and visible units of a network\n",
        "randomly to prevent overfitting of data (typically dropping 20 percent of the nodes).\n",
        "It doubles the number of iterations needed to converge the network.\n",
        "Batch normalization is the technique to improve the performance and stability of\n",
        "neural networks by normalizing the inputs in every layer so that they have mean\n",
        "output activation of zero and standard deviation of one.\n",
        "16. What Is the Difference Between Batch Gradient Descent and Stochastic\n",
        "Gradient Descent?\n",
        "Batch Gradient Descent\n",
        "Stochastic Gradient Descent\n",
        "The batch gradient computes the gradient using the\n",
        "entire dataset.\n",
        "It takes time to converge because the volume of data is\n",
        "huge, and weights update slowly.\n",
        "The stochastic gradient\n",
        "computes the gradient using a\n",
        "single sample.\n",
        "It converges much faster than\n",
        "the batch gradient because it\n",
        "updates weight more frequently.\n",
        "17. What is Overfitting and Underfitting, and How to Combat Them?\n",
        "Overfitting occurs when the model learns the details and noise in the training data to\n",
        "the degree that it adversely impacts the execution of the model on new information.\n",
        "It is more likely to occur with nonlinear models that have more flexibility when\n",
        "learning a target function. An example would be if a model is looking at cars and\n",
        "trucks, but only recognizes trucks that have a specific box shape. It might not be\n",
        "able to notice a flatbed truck because there's only a particular kind of truck it saw in\n",
        "training. The model performs well on training data, but not in the real world.\n",
        "Underfitting alludes to a model that is neither well-trained on data nor can generalize\n",
        "to new information. This usually happens when there is less and incorrect data to\n",
        "train a model. Underfitting has both poor performance and accuracy.\n",
        "To combat overfitting and underfitting, you can resample the data to estimate the\n",
        "model accuracy (k-fold cross-validation) and by having a validation dataset to\n",
        "evaluate the model.\n",
        "18. How Are Weights Initialized in a Network?\n",
        "There are two methods here: we can either initialize the weights to zero or assign\n",
        "them randomly.\n",
        "Initializing all weights to 0: This makes your model similar to a linear model. All the\n",
        "neurons and every layer perform the same operation, giving the same output and\n",
        "making the deep net useless.\n",
        "Initializing all weights randomly: Here, the weights are assigned randomly by\n",
        "initializing them very close to 0. It gives better accuracy to the model since every\n",
        "neuron performs different computations. This is the most commonly used method.\n",
        "19. What Are the Different Layers on CNN?\n",
        "There are four layers in CNN:\n",
        "1. Convolutional Layer -  the layer that performs a convolutional operation,\n",
        "creating several smaller picture windows to go over the data.\n",
        "2. ReLU Layer - it brings non-linearity to the network and converts all the\n",
        "negative pixels to zero. The output is a rectified feature map.\n",
        "3. Pooling Layer - pooling is a down-sampling operation that reduces the\n",
        "dimensionality of the feature map.\n",
        "4. Fully Connected Layer - this layer recognizes and classifies the objects in\n",
        "the image.\n",
        "20. What is Pooling on CNN, and How Does It Work?\n",
        "Pooling is used to reduce the spatial dimensions of a CNN. It performs down\n",
        "sampling operations to reduce the dimensionality and creates a pooled feature map\n",
        "by sliding a filter matrix over the input matrix.\n",
        "21. How Does an LSTM Network Work?\n",
        "Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network\n",
        "capable of learning long-term dependencies, remembering information for long\n",
        "periods as its default behavior. There are three steps in an LSTM network:\n",
        "• Step 1: The network decides what to forget and what to remember.\n",
        "• Step 2: It selectively updates cell state values.\n",
        "• Step 3: The network decides what part of the current state makes it to the\n",
        "output.\n",
        "22. What Are Vanishing and Exploding Gradients?\n",
        "While training an RNN, your slope can become either too small or too large; this\n",
        "makes the training difficult. When the slope is too small, the problem is known as a\n",
        "“Vanishing Gradient.” When the slope tends to grow exponentially instead of\n",
        "decaying, it’s referred to as an “Exploding Gradient.” Gradient problems lead to long\n",
        "training times, poor performance, and low accuracy.\n",
        "23. What Is the Difference Between Epoch, Batch, and Iteration in Deep\n",
        "Learning?\n",
        "• Epoch - Represents one iteration over the entire dataset (everything put\n",
        "into the training model).\n",
        "• Batch - Refers to when we cannot pass the entire dataset into the neural\n",
        "network at once, so we divide the dataset into several batches.\n",
        "• Iteration - if we have 10,000 images as data and a batch size of 200. then\n",
        "an epoch should run 50 iterations (10,000 divided by 50).\n",
        "24. Why is Tensorflow the Most Preferred Library in Deep Learning?\n",
        "Tensorflow provides both C++ and Python APIs, making it easier to work on and has\n",
        "a faster compilation time compared to other Deep Learning libraries like Keras and\n",
        "Torch. Tensorflow supports both CPU and GPU computing devices.\n",
        "25. What Do You Mean by Tensor in Tensorflow?\n",
        "A tensor is a mathematical object represented as arrays of higher dimensions.\n",
        "These arrays of data with different dimensions and ranks fed as input to the neural\n",
        "network are called “Tensors.”\n",
        "26. What Are the Programming Elements in Tensorflow?\n",
        "Constants - Constants are parameters whose value does not change. To define a\n",
        "constant we use  tf.constant() command. For example:\n",
        "a = tf.constant(2.0,tf.float32)\n",
        "b = tf.constant(3.0)\n",
        "Print(a, b)\n",
        "Variables - Variables allow us to add new trainable parameters to graph. To define a\n",
        "variable, we use the tf.Variable() command and initialize them before running the\n",
        "graph in a session. An example:\n",
        "W = tf.Variable([.3].dtype=tf.float32)\n",
        "b = tf.Variable([-.3].dtype=tf.float32)\n",
        "Placeholders - these allow us to feed data to a tensorflow model from outside a\n",
        "model. It permits a value to be assigned later. To define a placeholder, we use the\n",
        "tf.placeholder() command. An example:\n",
        "a = tf.placeholder (tf.float32)\n",
        "b = a*2\n",
        "with tf.Session() as sess:\n",
        "result = sess.run(b,feed_dict={a:3.0})\n",
        "print result\n",
        "Sessions - a session is run to evaluate the nodes. This is called the “Tensorflow\n",
        "runtime.” For example:\n",
        "a = tf.constant(2.0)\n",
        "b = tf.constant(4.0)\n",
        "c = a+b\n",
        "# Launch Session\n",
        "Sess = tf.Session()\n",
        "# Evaluate the tensor c\n",
        "print(sess.run(c))\n",
        "27. Explain a Computational Graph.\n",
        "Everything in a tensorflow is based on creating a computational graph. It has a\n",
        "network of nodes where each node operates, Nodes represent mathematical\n",
        "operations, and edges represent tensors. Since data flows in the form of a graph, it is\n",
        "also called a “DataFlow Graph.”\n",
        "28. Explain Generative Adversarial Network.\n",
        "Suppose there is a wine shop purchasing wine from dealers, which they resell later.\n",
        "But some dealers sell fake wine. In this case, the shop owner should be able to\n",
        "distinguish between fake and authentic wine.\n",
        "The forger will try different techniques to sell fake wine and make sure specific\n",
        "techniques go past the shop owner’s check. The shop owner would probably get\n",
        "some feedback from wine experts that some of the wine is not original. The owner\n",
        "would have to improve how he determines whether a wine is fake or authentic.\n",
        "The forger’s goal is to create wines that are indistinguishable from the authentic\n",
        "ones while the shop owner intends to tell if the wine is real or not accurately.\n",
        "Let us understand this example with the help of an image shown above.\n",
        "There is a noise vector coming into the forger who is generating fake wine.\n",
        "Here the forger acts as a Generator.\n",
        "The shop owner acts as a Discriminator.\n",
        "The Discriminator gets two inputs; one is the fake wine, while the other is the real\n",
        "authentic wine. The shop owner has to figure out whether it is real or fake.\n",
        "So, there are two primary components of Generative Adversarial Network (GAN)\n",
        "named:\n",
        "1. Generator\n",
        "2. Discriminator\n",
        "The generator is a CNN that keeps keys producing images and is closer in\n",
        "appearance to the real images while the discriminator tries to determine the\n",
        "difference between real and fake images The ultimate aim is to make the\n",
        "discriminator learn to identify real and fake images.\n",
        "29. What Is an Auto-encoder?\n",
        "This Neural Network has three layers in which the input neurons are equal to the\n",
        "output neurons. The network's target outside is the same as the input. It uses\n",
        "dimensionality reduction to restructure the input. It works by compressing the image\n",
        "input to a latent space representation then reconstructing the output from this\n",
        "representation.\n",
        "30. What Is Bagging and Boosting?\n",
        "Bagging and Boosting are ensemble techniques to train multiple models using the\n",
        "same learning algorithm and then taking a call.\n",
        "With Bagging, we take a dataset and split it into training data and test data. Then we\n",
        "randomly select data to place into the bags and train the model separately.\n",
        "With Boosting, the emphasis is on selecting data points which give wrong output to\n",
        "improve the accuracy.\n",
        "31. What is the difference between a Perceptron and Logistic\n",
        "Regression?\n",
        "A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for\n",
        "classification. For a binary classification problem, we know that the output can be either\n",
        "0 or 1. This is just like our simple logistic regression, where we use a logit function to\n",
        "generate a probability between 0 and 1.\n",
        "So, what’s the difference between the two?\n",
        "Simply put, it is just the difference in the threshold function! When we restrict the logistic\n",
        "regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:\n",
        "32. Can we have the same bias for all neurons of a hidden\n",
        "layer?\n",
        "Essentially, you can have a different bias value at each layer or at each neuron as well.\n",
        "However, it is best if we have a bias matrix for all the neurons in the hidden layers as\n",
        "well.\n",
        "A point to note is that both these strategies would give you very different results.\n",
        "33. What if we do not use any activation function(s) in a neural\n",
        "network?\n",
        "The main aim of this question is to understand why we need activation functions in a\n",
        "neural network. You can start off by giving a simple explanation of how neural networks\n",
        "are built:\n",
        "Step 1: Calculate the sum of all the inputs (X) according to their weights and include the\n",
        "bias term:\n",
        "Z = (weights * X) + bias\n",
        "Step 2: Apply an activation function to calculate the expected output:\n",
        "Y = Activation(Z)\n",
        "Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward\n",
        "propagation! Now, what if there is no activation function?\n",
        "Our equation for Y essentially becomes:\n",
        "Y = Z = (weights * X) + bias\n",
        "Wait – isn’t this just a simple linear equation? Yes – and that is why we need activation\n",
        "functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.\n",
        "In order to capture non-linear relationships, we use activation functions, and that is why\n",
        "a neural network without an activation function is just a linear regression model.\n",
        "34. In a neural network, what if all the weights are initialized\n",
        "with the same value?\n",
        "In simplest terms, if all the neurons have the same value of weights, each hidden unit\n",
        "will get exactly the same signal. While this might work during forward propagation, the\n",
        "derivative of the cost function during backward propagation would be the same every\n",
        "time.\n",
        "In short, there is no learning happening by the network! What do you call the\n",
        "phenomenon of the model being unable to learn any patterns from the data?\n",
        "Yes, underfitting.\n",
        "Therefore, if all weights have the same initial value, this would lead to underfitting.\n",
        "Note: This question might further lead to questions on exploding and vanishing gradients,\n",
        "which I have covered below.\n",
        "35. List the supervised and unsupervised tasks in Deep\n",
        "Learning.\n",
        "Now, this can be one tricky question. There might be a misconception that deep learning\n",
        "can only solve unsupervised learning problems. This is not the case. Some example of\n",
        "Supervised Learning and Deep learning include:\n",
        "• Image classification\n",
        "• Text classification\n",
        "• Sequence tagging\n",
        "On the other hand, there are some unsupervised deep learning techniques as well:\n",
        "• Word embeddings (like Skip-gram and Continuous Bag of Words): Understanding\n",
        "Word Embeddings: From Word2Vec to Count Vectors\n",
        "• Autoencoders: Learn How to Enhance a Blurred Image using an Autoencoder!\n",
        "Here is a great article on applications of Deep Learning for unsupervised tasks:\n",
        "• Essentials of Deep Learning: Introduction to Unsupervised Deep Learning (with\n",
        "Python codes)\n",
        "36. What is the role of weights and bias in a neural network?\n",
        "This is a question best explained with a real-life example. Consider that you want to go\n",
        "out today to play a cricket match with your friends. Now, a number of factors can affect\n",
        "your decision-making, like:\n",
        "• How many of your friends can make it to the game?\n",
        "• How much equipment can all of you bring?\n",
        "• What is the temperature outside?\n",
        "And so on. These factors can change your decision greatly or not too much. For\n",
        "example, if it is raining outside, then you cannot go out to play at all. Or if you have only\n",
        "one bat, you can share it while playing as well. The magnitude by which these factors\n",
        "can affect the game is called the weight of that factor.\n",
        "Factors like the weather or temperature might have a higher weight, and other factors\n",
        "like equipment would have a lower weight.\n",
        "However, does this mean that we can play a cricket match with only one bat? No – we\n",
        "would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias\n",
        "lets you assign some threshold which helps you activate a decision-point (or a neuron)\n",
        "only when that threshold is crossed.\n",
        "37. How does forward propagation and backpropagation work\n",
        "in deep learning?\n",
        "Now, this can be answered in two ways. If you are on a phone interview, you cannot\n",
        "perform all the calculus in writing and show the interviewer. In such cases, it best to\n",
        "explain it as such:\n",
        "• Forward propagation: The inputs are provided with weights to the hidden layer. At\n",
        "each hidden layer, we calculate the output of the activation at each node and this\n",
        "further propagates to the next layer till the final output layer is reached. Since we\n",
        "start from the inputs to the final output layer, we move forward and it is called\n",
        "forward propagation\n",
        "• Backpropagation: We minimize the cost function by its understanding of how it\n",
        "changes with changing the weights and biases in a neural network. This change\n",
        "is obtained by calculating the gradient at each hidden layer (and using the chain\n",
        "rule). Since we start from the final cost function and go back each hidden layer,\n",
        "we move backward and thus it is called backward propagation\n",
        "For an in-person interview, it is best to take up the marker, create a simple neural\n",
        "network with 2 inputs, a hidden layer, and an output layer, and explain it.\n",
        "Forward propagation:\n",
        "Backpropagation:\n",
        "At layer L2, for all weights:\n",
        "At layer L1, for all weights:\n",
        "You need not explain with respect to the bias term as well, though you might need to\n",
        "expand the above equations substituting the actual derivatives.\n",
        "38. What are the common data structures used in Deep\n",
        "Learning?\n",
        "Deep Learning goes right from the simplest data structures like lists to complicated\n",
        "ones like computation graphs.\n",
        "Here are the most common ones:\n",
        "• List: An ordered sequence of elements (You can also mention NumPy ndarrays\n",
        "here)\n",
        "• Matrix: An ordered sequence of elements with rows and columns\n",
        "• Dataframe: A dataframe is just like a matrix, but it holds actual data with the\n",
        "column names and rows denoting each datapoint in your dataset. If marks of 100\n",
        "students, their grades, and their details are stored in a dataframe, their details are\n",
        "stored as columns. Each row will represent the data of each of the 100 students\n",
        "• Tensors: You will work with them on a daily basis if you have ventured into deep\n",
        "learning. Used both in PyTorch and TensorFlow, tensors are like the basic\n",
        "programming unit of deep learning. Just like multidimensional arrays, we can\n",
        "perform numerous mathematical operations on them. Read more about\n",
        "tensors here\n",
        "• Computation Graphs: Since deep learning involves multiple layers and often\n",
        "hundreds, if not thousands of parameters, it is important to understand the flow\n",
        "of computation. A computation graph is just that. A computation graph gives us\n",
        "the sequence of operations performed with each node denoting an operation or a\n",
        "component in the neural network\n",
        "39. Why should we use Batch Normalization?\n",
        "Once the interviewer has asked you about the fundamentals of deep learning\n",
        "architectures, they would move on to the key topic of improving your deep learning\n",
        "model’s performance.\n",
        "Batch Normalization is one of the techniques used for reducing the training time of our\n",
        "deep learning algorithm. Just like normalizing our input helps improve our logistic\n",
        "regression model, we can normalize the activations of the hidden layers in our deep\n",
        "learning model as well:\n",
        "We basically normalize a[1] and a[2] here. This means we normalize the inputs to the\n",
        "layer, and then apply the activation functions to the normalized inputs.\n",
        "Here is an article that explains Batch Normalization and other techniques for improving\n",
        "Neural Networks: Neural Networks – Hyperparameter Tuning, Regularization &\n",
        "Optimization.\n",
        "40. List the activation functions you have used so far in your\n",
        "projects and how you would choose one.\n",
        "The most common activation functions are:\n",
        "• Sigmoid\n",
        "• Tanh\n",
        "• ReLU\n",
        "• Softmax\n",
        "While it is not important to know all the activation functions, you can always score\n",
        "points by knowing the range of these functions and how they are used. Here is a handy\n",
        "table for you to follow:\n",
        "Here is a great guide on how to use these and other activations functions: Fundamentals\n",
        "of Deep Learning – Activation Functions and When to Use Them?.\n",
        "41. Why does a Convolutional Neural Network (CNN) work\n",
        "better with image data?\n",
        "The key to this question lies in the Convolution operation. Unlike humans, the machine\n",
        "sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or\n",
        "an ear, it just identifies curves and edges.\n",
        "Thus, instead of looking at the entire image, it helps to just read the image in parts.\n",
        "Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3\n",
        "matrices and dealing with them one by one. This is convolution.\n",
        "Mathematically, we just perform a small operation on the matrix to help us detect\n",
        "features in the image – like boundaries, colors, etc.\n",
        "Z = X * f\n",
        "Here, we are convolving (* operation – not multiplication) the input matrix X with another\n",
        "small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then\n",
        "passed on to the other layers.\n",
        "If you have a board/screen in front of you, you can always illustrate this with a simple\n",
        "example:\n",
        "Learning more about how CNNs work here.\n",
        "42. Why do RNNs work better with text data?\n",
        "The main component that differentiates Recurrent Neural Networks (RNN) from the\n",
        "other models is the addition of a loop at each node. This loop brings\n",
        "the recurrence mechanism in RNNs. In a basic Artificial Neural Network (ANN), each\n",
        "input is given the same weight and fed to the network at the same time. So, for a\n",
        "sentence like “I saw the movie and hated it”, it would be difficult to capture the\n",
        "information which associates “it” with the “movie”.\n",
        "The addition of a loop is to denote preserving the previous node’s information for the\n",
        "next node, and so on. This is why RNNs are much better for sequential data, and since\n",
        "text data also is sequential in nature, they are an improvement over ANNs.\n",
        "43. In a CNN, if the input size 5 X 5 and the filter size is 7 X 7,\n",
        "then what would be the size of the output?\n",
        "This is a pretty intuitive answer. As we saw above, we perform the convolution on ‘x’ one\n",
        "step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with\n",
        "dimensions 3 X 3.\n",
        "Thus, to make the input size similar to the filter size, we make use of padding – adding\n",
        "0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output\n",
        "size would be using the formula:\n",
        "Dimension of image = (n, n) = 5 X 5\n",
        "Dimension of filter = (f,f)  = 7 X 7\n",
        "Padding = 1 (adding 1 pixel with value 0 all around the edges)\n",
        "Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1\n",
        "44. What’s the difference between valid and same padding in a\n",
        "CNN?\n",
        "This question has more chances of being a follow-up question to the previous one. Or if\n",
        "you have explained how you used CNNs in a computer vision task, the interviewer might\n",
        "ask this question along with the details of the padding parameters.\n",
        "• Valid Padding: When we do not use any padding. The resultant matrix after\n",
        "convolution will have dimensions (n – f + 1) X (n – f + 1)\n",
        "• Same padding: Adding padded elements all around the edges such that the\n",
        "output matrix will have the same dimensions as that of the input matrix\n",
        "45. What do you mean by exploding and vanishing gradients?\n",
        "The key here is to make the explanation as simple as possible. As we know, the gradient\n",
        "descent algorithm tries to minimize the error by taking small steps towards the\n",
        "minimum value. These steps are used to update the weights and biases in a neural\n",
        "network.\n",
        "However, at times, the steps become too large and this results in larger updates to\n",
        "weights and bias terms – so much so as to cause an overflow (or a NaN) value in the\n",
        "weights. This leads to an unstable algorithm and is called an exploding gradient.\n",
        "On the other hand, the steps are too small and this leads to minimal changes in the\n",
        "weights and bias terms – even negligible changes at times. We thus might end up\n",
        "training a deep learning model with almost the same weights and biases each time and\n",
        "never reach the minimum error function. This is called the vanishing gradient.\n",
        "A point to note is that both these issues are specifically evident in Recurrent Neural\n",
        "Networks – so be prepared for follow-up questions on RNN!\n",
        "46. What are the applications of transfer learning in Deep\n",
        "Learning?\n",
        "I am sure you would have a doubt as to why a relatively simple question was included in\n",
        "the Intermediate Level. The reason is the sheer volume of subsequent questions it can\n",
        "generate!\n",
        "The use of transfer learning has been one of the key milestones in deep learning.\n",
        "Training a large model on a huge dataset, and then using the final parameters on smaller\n",
        "simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be\n",
        "it Computer Vision or NLP, pretrained models have become the norm in research and in\n",
        "the industry.\n",
        "Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.\n",
        "It is here that you can earn brownie points by pointing out specific examples/projects\n",
        "where you used these models and how you used them as well.\n",
        "It is not possible to discuss all of them, so here are a few resources to get started:\n",
        "47. How backpropagation is different in RNN compared to\n",
        "ANN?\n",
        "In Recurrent Neural Networks, we have an additional loop at each node:\n",
        "This loop essentially includes a time component into the network as well. This helps in\n",
        "capturing sequential information from the data, which could not be possible in a generic\n",
        "artificial neural network.\n",
        "This is why the backpropagation in RNN is called Backpropagation through Time, as in\n",
        "backpropagation at each time step.\n",
        "You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning –\n",
        "Introduction to Recurrent Neural Networks.\n",
        "48. How does LSTM solve the vanishing gradient challenge?\n",
        "The LSTM model is considered a special case of RNNs. The problems of vanishing\n",
        "gradients and exploding gradients we saw earlier are a disadvantage while using the\n",
        "plain RNN model.\n",
        "In LSTMs, we add a forget gate, which is basically a memory unit that retains\n",
        "information that is retained across timesteps and discards the other information that is\n",
        "not needed. This also necessitates the need for input and output gates to include the\n",
        "results of the forget gate as well.\n",
        "49. Why is GRU faster as compared to LSTM?\n",
        "As you can see, the LSTM model can become quite complex. In order to still retain the\n",
        "functionality of retaining information across time and yet not make a too complex\n",
        "model, we need GRUs.\n",
        "Basically, in GRUs, instead of having an additional Forget gate, we combine the input and\n",
        "Forget gates into a single Update Gate:\n",
        "It is this reduction in the number of gates that makes GRU less complex and faster than\n",
        "LSTM. You can learn about GRUs, LSTMs and other sequence models in detail\n",
        "here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models.\n",
        "50. How is the transformer architecture better than RNN?\n",
        "Advancements in deep learning have made it possible to solve many tasks in Natural\n",
        "Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are\n",
        "specifically used for this purpose – so as to capture all possible information from a\n",
        "given sentence, or a paragraph. However, sequential processing comes with its caveats:\n",
        "• It requires high processing power\n",
        "• It is difficult to execute in parallel because of its sequential nature\n",
        "This gave rise to the Transformer architecture. Transformers use what is called the\n",
        "attention mechanism. This basically means mapping dependencies between all the\n",
        "parts of a sentence.\n",
        "Here is an excellent article explaining transformers: How do Transformers Work in NLP?\n",
        "A Guide to the Latest State-of-the-Art Models.\n",
        "51. Describe a project you worked on and the tools/frameworks\n",
        "you used?\n",
        "Now, this is one question that is sure to be asked even if none of the above ones is\n",
        "asked in your deep learning interview. I have included it in the advanced section since\n",
        "you might be grilled on each and every part of the code you have written.\n",
        "Before the interview, make sure to:\n",
        "• have your GitHub code updated with the latest code changes you have made\n",
        "• be ready to give in-depth explanations on at least 2-3 projects where you used\n",
        "deep learning\n",
        "When you are asked such a question, it is best to give a small 30-second pitch on what\n",
        "was the:\n",
        "• problem statement\n",
        "• data you used and the framework (like PyTorch or TensorFlow)\n",
        "• any pretrained model you used or just the name of the basic model you built upon\n",
        "• the value of the evaluation metric you achieved\n",
        "After this, you can start going into detail about the model architecture, what\n",
        "preprocessing steps you had to take, and how that changed the data.\n",
        "An important point to be noted is that the project need not be a very complicated or\n",
        "sophisticated one. A well-explained object detection project would earn you more points\n",
        "than a poorly-explained video classification project. Towards this end, I recommend\n",
        "having a README file in the above format for every project that you have implemented.\n",
        "52) What is deep learning?\n",
        "Deep learning is a part of machine learning with an algorithm inspired by the\n",
        "structure and function of the brain, which is called an artificial neural network. In\n",
        "the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while\n",
        "working on deep learning network. Deep learning is suited over a range of fields\n",
        "such as computer vision, speech recognition, natural language processing, etc.\n",
        "53) What are the main differences between AI, Machine\n",
        "Learning, and Deep Learning?\n",
        "o AI stands for Artificial Intelligence. It is a technique which enables machines\n",
        "to mimic human behavior.\n",
        "o Machine Learning is a subset of AI which uses statistical methods to enable\n",
        "machines to improve with experiences.\n",
        "o Deep learning is a part of Machine learning, which makes the computation of\n",
        "multi-layer neural networks feasible. It takes advantage of neural networks to\n",
        "simulate human-like decision making.\n",
        "54) Differentiate supervised and unsupervised deep learning\n",
        "procedures.\n",
        "o Supervised learning is a system in which both input and desired output data\n",
        "are provided. Input and output data are labeled to provide a learning basis for\n",
        "future data processing.\n",
        "o Unsupervised procedure does not need labeling information explicitly, and the\n",
        "operations can be carried out without the same. The common unsupervised\n",
        "learning method is cluster analysis. It is used for exploratory data analysis\n",
        "to find hidden patterns or grouping in data.\n",
        "55) What are the applications of deep learning?\n",
        "There are various applications of deep learning:\n",
        "o Computer vision\n",
        "o Natural language processing and pattern recognition\n",
        "o Image recognition and processing\n",
        "o Machine translation\n",
        "o Sentiment analysis\n",
        "o Question Answering system\n",
        "o Object Classification and Detection\n",
        "o Automatic Handwriting Generation\n",
        "o Automatic Text Generation.\n",
        "56) Do you think that deep network is better than a shallow\n",
        "one?\n",
        "Both shallow and deep networks are good enough and capable of approximating any\n",
        "function. But for the same level of accuracy, deeper networks can be much more\n",
        "efficient in terms of computation and number of parameters. Deeper networks can\n",
        "create deep representations. At every layer, the network learns a new, more\n",
        "abstract representation of the input.\n",
        "57) What do you mean by \"overfitting\"?\n",
        "Overfitting is the most common issue which occurs in deep learning. It usually\n",
        "occurs when a deep learning algorithm apprehends the sound of specific data. It also\n",
        "appears when the particular algorithm is well suitable for the data and shows up\n",
        "when the algorithm or model represents high variance and low bias.\n",
        "58) What is Backpropagation?\n",
        "Backpropagation is a training algorithm which is used for multilayer neural networks.\n",
        "It transfers the error information from the end of the network to all the weights\n",
        "inside the network. It allows the efficient computation of the gradient.\n",
        "Backpropagation can be divided into the following steps:\n",
        "o It can forward propagation of training data through the network to generate\n",
        "output.\n",
        "o It uses target value and output value to compute error derivative concerning\n",
        "output activations.\n",
        "o It can backpropagate to compute the derivative of the error concerning\n",
        "output activations in the previous layer and continue for all hidden layers.\n",
        "o It uses the previously calculated derivatives for output and all hidden layers\n",
        "to calculate the error derivative concerning weights.\n",
        "o It updates the weights.\n",
        "59) What is the function of the Fourier Transform in Deep\n",
        "Learning?\n",
        "Fourier transform package is highly efficient for analyzing, maintaining, and\n",
        "managing a large databases. The software is created with a high-quality feature\n",
        "known as the special portrayal. One can effectively utilize it to generate real-time\n",
        "array data, which is extremely helpful for processing all categories of signals.\n",
        "60) Describe the theory of autonomous form of deep learning in\n",
        "a few words.\n",
        "There are several forms and categories available for the particular subject, but the\n",
        "autonomous pattern represents independent or unspecified mathematical bases\n",
        "which are free from any specific categorizer or formula.\n",
        "62) What is the use of Deep learning in today's age, and how is\n",
        "it adding data scientists?\n",
        "Deep learning has brought significant changes or revolution in the field of machine\n",
        "learning and data science. The concept of a complex neural network (CNN) is the\n",
        "main center of attention for data scientists. It is widely taken because of its\n",
        "advantages in performing next-level machine learning operations. The advantages of\n",
        "deep learning also include the process of clarifying and simplifying issues based on\n",
        "an algorithm due to its utmost flexible and adaptable nature. It is one of the rare\n",
        "procedures which allow the movement of data in independent pathways. Most of the\n",
        "data scientists are viewing this particular medium as an advanced additive and\n",
        "extended way to the existing process of machine learning and utilizing the same for\n",
        "solving complex day to day issues.\n",
        "63) What are the deep learning frameworks or tools?\n",
        "Deep learning frameworks or tools are:\n",
        "Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK,\n",
        "DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL\n",
        "64) What are the disadvantages of deep learning?\n",
        "There are some disadvantages of deep learning, which are:\n",
        "o Deep learning model takes longer time to execute the model. In some cases,\n",
        "it even takes several days to execute a single model depends on complexity.\n",
        "o The deep learning model is not good for small data sets, and it fails here.\n",
        "65) What is the meaning of term weight initialization in neural\n",
        "networks?\n",
        "In neural networking, weight initialization is one of the essential factors. A bad\n",
        "weight initialization prevents a network from learning. On the other side, a good\n",
        "weight initialization helps in giving a quicker convergence and a better overall error.\n",
        "Biases can be initialized to zero. The standard rule for setting the weights is to be\n",
        "close to zero without being too small.\n",
        "66) Explain Data Normalization.\n",
        "Data normalization is an essential preprocessing step, which is used to rescale\n",
        "values to fit in a specific range. It assures better convergence during\n",
        "backpropagation. In general, data normalization boils down to subtracting the mean\n",
        "of each data point and dividing by its standard deviation.\n",
        "67) Why is zero initialization not a good weight initialization\n",
        "process?\n",
        "If the set of weights in the network is put to a zero, then all the neurons at each\n",
        "layer will start producing the same output and the same gradients during\n",
        "backpropagation.\n",
        "As a result, the network cannot learn at all because there is no source of asymmetry\n",
        "between neurons. That is the reason why we need to add randomness to the weight\n",
        "initialization process.\n",
        "68) What are the prerequisites for starting in Deep Learning?\n",
        "There are some basic requirements for starting in Deep Learning, which are:\n",
        "o Machine Learning\n",
        "o Mathematics\n",
        "o Python Programming\n",
        "69) What are the supervised learning algorithms in Deep\n",
        "learning?\n",
        "o Artificial neural network\n",
        "o Convolution neural network\n",
        "o Recurrent neural network\n",
        "70) What are the unsupervised learning algorithms in Deep\n",
        "learning?\n",
        "o Self Organizing Maps\n",
        "o Deep belief networks (Boltzmann Machine)\n",
        "o Auto Encoders\n",
        "71) How many layers in the neural network?\n",
        "o Input Layer\n",
        "The input layer contains input neurons which send information to the hidden\n",
        "layer.\n",
        "o Hidden Layer\n",
        "The hidden layer is used to send data to the output layer.\n",
        "o Output Layer\n",
        "The data is made available at the output layer.\n",
        "72) What is the use of the Activation function?\n",
        "The activation function is used to introduce nonlinearity into the neural network so\n",
        "that it can learn more complex function. Without the Activation function, the neural\n",
        "network would be only able to learn function, which is a linear combination of its\n",
        "input data.\n",
        "Activation function translates the inputs into outputs. The activation function is\n",
        "responsible for deciding whether a neuron should be activated or not. It makes the\n",
        "decision by calculating the weighted sum and further adding bias with it. The basic\n",
        "purpose of the activation function is to introduce non-linearity into the output of a\n",
        "neuron.\n",
        "73) How many types of activation function are available?\n",
        "o Binary Step\n",
        "o Sigmoid\n",
        "o Tanh\n",
        "o ReLU\n",
        "o Leaky ReLU\n",
        "o Softmax\n",
        "o Swish\n",
        "74) What is a binary step function?\n",
        "The binary step function is an activation function, which is usually based on a\n",
        "threshold. If the input value is above or below a particular threshold limit, the\n",
        "neuron is activated, then it sends the same signal to the next layer. This function\n",
        "does not allow multi-value outputs.\n",
        "75) What is the sigmoid function?\n",
        "The sigmoid activation function is also called the logistic function. It is traditionally a\n",
        "trendy activation function for neural networks. The input data to the function is\n",
        "transformed into a value between 0.0 and 1.0. Input values that are much larger\n",
        "than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller\n",
        "than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is\n",
        "an S-shape from zero up through 0.5 to 1.0. It was the default activation used on\n",
        "neural networks, in the early 1990s.\n",
        "76) What is Tanh function?\n",
        "The hyperbolic tangent function, also known as tanh for short, is a similar shaped\n",
        "nonlinear activation function. It provides output values between -1.0 and 1.0. Later\n",
        "in the 1990s and through the 2000s, this function was preferred over the sigmoid\n",
        "activation function as models. It was easier to train and often had better predictive\n",
        "performance.\n",
        "77) What is ReLU function?\n",
        "A node or unit which implements the activation function is referred to as a rectified\n",
        "linear activation unit or ReLU for short. Generally, networks that use the rectifier\n",
        "function for the hidden layers are referred to as rectified networks.\n",
        "Adoption of ReLU may easily be considered one of the few milestones in the deep\n",
        "learning revolution.\n",
        "78) What is the use of leaky ReLU function?\n",
        "The Leaky ReLU (LReLU or LReL) manages the function to allow small negative\n",
        "values when the input is less than zero.\n",
        "79) What is the softmax function?\n",
        "The softmax function is used to calculate the probability distribution of the event\n",
        "over 'n' different events. One of the main advantages of using softmax is the output\n",
        "probabilities range. The range will be between 0 to 1, and the sum of all the\n",
        "probabilities will be equal to one. When the softmax function is used for multi\n",
        "classification model, it returns the probabilities of each class, and the target class\n",
        "will have a high probability.\n",
        "80) What is a Swish function?\n",
        "Swish is a new, self-gated activation function. Researchers at Google discovered the\n",
        "Swish function. According to their paper, it performs better than ReLU with a similar\n",
        "level of computational efficiency.\n",
        "81) What is the most used activation function?\n",
        "Relu function is the most used activation function. It helps us to solve vanishing\n",
        "gradient problems.\n",
        "82) Can Relu function be used in output layer?\n",
        "No, Relu function has to be used in hidden layers.\n",
        "83) In which layer softmax activation function used?\n",
        "Softmax activation function has to be used in the output layer.\n",
        "84) What do you understand by Autoencoder?\n",
        "Autoencoder is an artificial neural network. It can learn representation for a set of\n",
        "data without any supervision. The network automatically learns by copying its input\n",
        "to the output; typically,internet representation consists of smaller dimensions than\n",
        "the input vector. As a result, they can learn efficient ways of representing the data.\n",
        "Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal\n",
        "representation, and a decoder converts the internal state to the outputs.\n",
        "85) What do you mean by Dropout?\n",
        "Dropout is a cheap regulation technique used for reducing overfitting in neural\n",
        "networks. We randomly drop out a set of nodes at each training step. As a result,\n",
        "we create a different model for each training case, and all of these models share\n",
        "weights. It's a form of model averaging.\n",
        "86) What do you understand by Tensors?\n",
        "Tensors are nothing but a de facto for representing the data in deep learning. They\n",
        "are just multidimensional arrays, which allows us to represent the data having\n",
        "higher dimensions. In general, we deal with high dimensional data sets where\n",
        "dimensions refer to different features present in the data set.\n",
        "87) What do you understand by Boltzmann Machine?\n",
        "A Boltzmann machine (also known as stochastic Hopfield network with hidden units)\n",
        "is a type of recurrent neural network. In a Boltzmann machine, nodes make binary\n",
        "decisions with some bias. Boltzmann machines can be strung together to create\n",
        "more sophisticated systems such as deep belief networks. Boltzmann Machines can\n",
        "be used to optimize the solution to a problem.\n",
        "Some important points about Boltzmann Machine-\n",
        "o It uses a recurrent structure.\n",
        "o It consists of stochastic neurons, which include one of the two possible states,\n",
        "either 1 or 0.\n",
        "o The neurons present in this are either in an adaptive state (free state) or\n",
        "clamped state (frozen state).\n",
        "o If we apply simulated annealing or discrete Hopfield network, then it would\n",
        "become a Boltzmann Machine.\n",
        "88) What is Model Capacity?\n",
        "The capacity of a deep learning neural network controls the scope of the types of\n",
        "mapping functions that it can learn. Model capacity can approximate any given\n",
        "function. When there is a higher model capacity, it means that the larger amount of\n",
        "information can be stored in the network.\n",
        "89) What is the cost function?\n",
        "A cost function describes us how well the neural network is performing with respect\n",
        "to its given training sample and the expected output. It may depend on variables\n",
        "such as weights and biases.It provides the performance of a neural network as a\n",
        "whole. In deep learning, our priority is to minimize the cost function. That's why we\n",
        "prefer to use the concept of gradient descent.\n",
        "90) Explain gradient descent?\n",
        "An optimization algorithm that is used to minimize some function by repeatedly\n",
        "moving in the direction of steepest descent as specified by the negative of the\n",
        "gradient is known as gradient descent. It's an iteration algorithm, in every iteration\n",
        "algorithm, we compute the gradient of a cost function, concerning each parameter\n",
        "and update the parameter of the function via the following formula:\n",
        "Where,\n",
        "Θ - is the parameter vector,\n",
        "α - learning rate,\n",
        "J(Θ) - is a cost function\n",
        "In machine learning, it is used to update the parameters of our model. Parameters\n",
        "represent the coefficients in linear regression and weights in neural networks.\n",
        "91) Explain the following variant of Gradient Descent:\n",
        "Stochastic, Batch, and Mini-batch?\n",
        "o Stochastic Gradient Descent\n",
        "Stochastic gradient descent is used to calculate the gradient and update the\n",
        "parameters by using only a single training example.\n",
        "o Batch Gradient Descent\n",
        "Batch gradient descent is used to calculate the gradients for the whole\n",
        "dataset and perform just one update at each iteration.\n",
        "o Mini-batch Gradient Descent\n",
        "Mini-batch gradient descent is a variation of stochastic gradient descent.\n",
        "Instead of a single training example, mini-batch of samples is used. Mini\n",
        "batch gradient descent is one of the most popular optimization algorithms.\n",
        "92) What are the main benefits of Mini-batch Gradient\n",
        "Descent?\n",
        "o It is computationally efficient compared to stochastic gradient descent.\n",
        "o It improves generalization by finding flat minima.\n",
        "o It improves convergence by using mini-batches. We can approximate the\n",
        "gradient of the entire training set, which might help to avoid local minima.\n",
        "93) What is matrix element-wise multiplication? Explain with an\n",
        "example.\n",
        "Element-wise matrix multiplication is used to take two matrices of the same\n",
        "dimensions. It further produces another combined matrix with the elements that are\n",
        "a product of corresponding elements of matrix a and b.\n",
        "94) What do you understand by a convolutional neural\n",
        "network?\n",
        "A convolutional neural network, often called CNN, is a feedforward neural network.\n",
        "It uses convolution in at least one of its layers. The convolutional layer contains a\n",
        "set of filter (kernels). This filter is sliding across the entire input image, computing\n",
        "the dot product between the weights of the filter and the input image. As a result of\n",
        "training, the network automatically learns filters that can detect specific features.\n",
        "95) Explain the different layers of CNN.\n",
        "There are four layered concepts that we should understand in CNN (Convolutional\n",
        "Neural Network):\n",
        "o Convolution\n",
        "This layer comprises of a set of independent filters. All these filters are\n",
        "initialized randomly. These filters then become our parameters which will be\n",
        "learned by the network subsequently.\n",
        "o ReLU\n",
        "The ReLu layer is used with the convolutional layer.\n",
        "o Pooling\n",
        "It reduces the spatial size of the representation to lower the number of\n",
        "parameters and computation in the network. This layer operates on each\n",
        "feature map independently.\n",
        "o Full Collectedness\n",
        "Neurons in a completely connected layer have complete connections to all\n",
        "activations in the previous layer, as seen in regular Neural Networks. Their\n",
        "activations can be easily computed with a matrix multiplication followed by a\n",
        "bias offset.\n",
        "96) What is an RNN?\n",
        "RNN stands for Recurrent Neural Networks. These are the artificial neural networks\n",
        "which are designed to recognize patterns in sequences of data such as handwriting,\n",
        "text, the spoken word, genomes, and numerical time series data. RNN use\n",
        "backpropagation algorithm for training because of their internal memory. RNN can\n",
        "remember important things about the input they received, which enables them to be\n",
        "very precise in predicting what's coming next.\n",
        "97) What are the issues faced while training in Recurrent\n",
        "Networks?\n",
        "Recurrent Neural Network uses backpropagation algorithm for training, but it is\n",
        "applied on every timestamp. It is usually known as Back-propagation Through\n",
        "Time (BTT).\n",
        "There are two significant issues with Back-propagation, such as:\n",
        "o Vanishing Gradient\n",
        "When we perform Back-propagation, the gradients tend to get smaller and\n",
        "smaller because we keep on moving backward in the Network. As a result,\n",
        "the neurons in the earlier layer learn very slowly if we compare it with the\n",
        "neurons in the later layers.Earlier layers are more valuable because they are\n",
        "responsible for learning and detecting simple patterns. They are the building\n",
        "blocks of the network.\n",
        "If they provide improper or inaccurate results, then how can we expect the\n",
        "next layers and complete network to perform nicely and provide accurate\n",
        "results. The training procedure tales long, and the prediction accuracy of the\n",
        "model decreases.\n",
        "o Exploding Gradient\n",
        "Exploding gradients are the main problem when large error gradients\n",
        "accumulate. They provide result in very large updates to neural network\n",
        "model weights during training.\n",
        "Gradient Descent process works best when updates are small and controlled.\n",
        "When the magnitudes of the gradient accumulate, an unstable network is\n",
        "likely to occur. It can cause poor prediction of results or even a model that\n",
        "reports nothing useful.\n",
        "98) Explain the importance of LSTM.\n",
        "LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent\n",
        "Neural Network) architecture, which is used in the field of deep learning. LSTM has\n",
        "feedback connections which makes it a \"general purpose computer.\" It can process\n",
        "not only a single data point but also entire sequences of data.\n",
        "They are a special kind of RNN which are capable of learning long-term\n",
        "dependencies.\n",
        "99) What are the different layers of Autoencoders? Explain\n",
        "briefly.\n",
        "An autoencoder contains three layers:\n",
        "o Encoder\n",
        "The encoder is used to compress the input into a latent space representation.\n",
        "It encodes the input images as a compressed representation in a reduced\n",
        "dimension. The compressed images are the distorted version of the original\n",
        "image.\n",
        "o Code\n",
        "The code layer is used to represent the compressed input which is fed to the\n",
        "decoder.\n",
        "o Decoder\n",
        "The decoder layer decodes the encoded image back to its original dimension.\n",
        "The decoded image is a reduced reconstruction of the original image. It is\n",
        "automatically reconstructed from the latent space representation.\n",
        "100) What do you understand by Deep Autoencoders?\n",
        "Deep Autoencoder is the extension of the simple Autoencoder. The first layer\n",
        "present in DeepAutoencoder is responsible for first-order functions in the raw input.\n",
        "The second layer is responsible for second-order functions corresponding to patterns\n",
        "in the appearance of first-order functions. Deeper layers which are available in the\n",
        "Deep Autoencoder tend to learn even high-order features.\n",
        "A deep autoencoder is the combination of two, symmetrical deep-belief networks:\n",
        "o First four or five shallow layers represent the encoding half.\n",
        "o The other combination of four or five layers makes up the decoding half.\n",
        "102) What are the three steps to developing the necessary\n",
        "assumption structure in Deep learning?\n",
        "The procedure of developing an assumption structure involves three specific actions.\n",
        "o The first step contains algorithm development. This particular process is\n",
        "lengthy.\n",
        "o The second step contains algorithm analyzing, which represents the in\n",
        "process methodology.\n",
        "o The third step is about implementing the general algorithm in the final\n",
        "procedure. The entire framework is interlinked and required for throughout\n",
        "the process.\n",
        "103) What do you understand by Perceptron? Also, explain its\n",
        "type.\n",
        "A perceptron is a neural network unit (an artificial neuron) that does certain\n",
        "computations to detect features. It is an algorithm for supervised learning of binary\n",
        "classifiers. This algorithm is used to enable neurons to learn and processes elements\n",
        "in the training set one at a time.\n",
        "There are two types of perceptrons:\n",
        "o Single-Layer Perceptron\n",
        "Single layer perceptrons can learn only linearly separable patterns.\n",
        "o Multilayer Perceptrons\n",
        "Multilayer perceptrons or feedforward neural networks with two or more\n",
        "layers have the higher processing power.\n",
        "105. Why is it necessary to introduce non-linearities in a neural\n",
        "network?\n",
        "Solution: otherwise, we would have a composition of linear\n",
        "functions, which is also a linear function, giving a linear model.\n",
        "A linear model has a much smaller number of parameters, and\n",
        "is therefore limited in the complexity it can model.\n",
        "106. Describe two ways of dealing with the vanishing gradient\n",
        "problem in a neural network.\n",
        "Solution:\n",
        "• Using ReLU activation instead of sigmoid.\n",
        "• Using Xavier initialization.\n",
        "107. What are some advantages in using a CNN (convolutional\n",
        "neural network) rather than a DNN (dense neural network) in\n",
        "an image classification task?\n",
        "Solution: while both models can capture the relationship\n",
        "between close pixels, CNNs have the following properties:\n",
        "• It is translation invariant — the exact location of the\n",
        "pixel is irrelevant for the filter.\n",
        "• It is less likely to overfit — the typical number of\n",
        "parameters in a CNN is much smaller than that of a\n",
        "DNN.\n",
        "• Gives us a better understanding of the model — we can\n",
        "look at the filters’ weights and visualize what the\n",
        "network “learned”.\n",
        "• Hierarchical nature — learns patterns in by describing\n",
        "complex patterns using simpler ones.\n",
        "108. Describe two ways to visualize features of a CNN in an\n",
        "image classification task.\n",
        "Solution:\n",
        "• Input occlusion — cover a part of the input image and\n",
        "see which part affect the classification the most. For\n",
        "instance, given a trained image classification model,\n",
        "give the images below as input. If, for instance, we see\n",
        "that the 3rd image is classified with 98% probability as\n",
        "a dog, while the 2nd image only with 65% accuracy, it\n",
        "means that the part covered in the 2nd image is more\n",
        "important.\n",
        "• Activation Maximization — the idea is to create an\n",
        "artificial input image that maximize the target response\n",
        "(gradient ascent).\n",
        "109. Is trying the following learning rates: 0.1,0.2,…,0.5 a good\n",
        "strategy to optimize the learning rate?\n",
        "Solution: No, it is recommended to try a logarithmic scale to\n",
        "optimize the learning rate.\n",
        "110. Suppose you have a NN with 3 layers and ReLU\n",
        "activations. What will happen if we initialize all the weights with\n",
        "the same value? what if we only had 1 layer (i.e linear/logistic\n",
        "regression?)\n",
        "Solution: If we initialize all the weights to be the same we\n",
        "would not be able to break the symmetry; i.e, all gradients will\n",
        "be updated the same and the network will not be able to learn.\n",
        "In the 1-layers scenario, however, the cost function is convex\n",
        "(linear/sigmoid) and thus the weights will always converge to\n",
        "the optimal point, regardless of the initial value (convergence\n",
        "may be slower).\n",
        "111. Explain the idea behind the Adam optimizer.\n",
        "Solution: Adam, or adaptive momentum, combines two ideas\n",
        "to improve convergence: per-parameter updates which give\n",
        "faster convergence, and momentum which helps to avoid\n",
        "getting stuck in saddle point.\n",
        "112. Compare batch, mini-batch and stochastic gradient\n",
        "descent.\n",
        "Solution: batch refers to estimating the data by taking the entire\n",
        "data, mini-batch by sampling a few datapoints, and SGD refers\n",
        "to update the gradient one datapoint at each epoch. The tradeoff\n",
        "here is between how precise the calculation of the gradient is\n",
        "versus what size of batch we can keep in memory. Moreover,\n",
        "taking mini-batch rather than the entire batch has a regularizing\n",
        "effect by adding random noise at each epoch.\n",
        "113. What is data augmentation? Give examples.\n",
        "Solution: Data augmentation is a technique to increase the\n",
        "input data by performing manipulations on the original data.\n",
        "For instance in images, one can: rotate the image, reflect (flip)\n",
        "the image, add Gaussian blur.\n",
        "114. What is the idea behind GANs?\n",
        "Solution: GANs, or generative adversarial networks, consist of\n",
        "two networks (D,G) where D is the “discriminator” network and\n",
        "G is the “generative” network. The goal is to create data —\n",
        "images, for instance, which are undistinguishable from real\n",
        "images. Suppose we want to create an adversarial example of a\n",
        "cat. The network G will generate images. The network D will\n",
        "classify images according to whether they are a cat or not. The\n",
        "cost function of G will be constructed such that it tries to “fool”\n",
        "D — to classify its output always as cat.\n",
        "115. What are the advantages of using Batchnorm?\n",
        "Solution: Batchnorm accelerates the training process. It also\n",
        "(as a byproduct of including some noise) has a regularizing\n",
        "effect.\n",
        "116. What is multi-task learning? When should it be used?\n",
        "Solution: Multi-tasking is useful when we have a small amount\n",
        "of data for some task, and we would benefit from training a\n",
        "model on a large dataset of another task. Parameters of the\n",
        "models are shared — either in a “hard” way (i.e the same\n",
        "parameters) or a “soft” way (i.e regularization/penalty to the\n",
        "cost function).\n",
        "117. What is end-to-end learning? Give a few of its advantages.\n",
        "Solution: End-to-end learning is usually a model which gets\n",
        "the raw data and outputs directly the desired outcome, with no\n",
        "intermediate tasks or feature engineering. It has several\n",
        "advantages, among which: there is no need to handcraft\n",
        "features, and it generally leads to lower bias.\n",
        "118. What happens if we use a ReLU activation and then a\n",
        "sigmoid as the final layer?\n",
        "Solution: Since ReLU always outputs a non-negative result,\n",
        "the network will constantly predict one class for all the inputs!\n",
        "119. How to solve the exploding gradient problem?\n",
        "Solution: A simple solution to the exploding gradient problem\n",
        "is gradient clipping — taking the gradient to be ±M when its\n",
        "absolute value is bigger than M, where M is some large number.\n",
        "120. Is it necessary to shuffle the training data when using\n",
        "batch gradient descent?\n",
        "Solution: No, because the gradient is calculated at each epoch\n",
        "using the entire training data, so shuffling does not make a\n",
        "difference.\n",
        "121. When using mini batch gradient descent, why is it\n",
        "important to shuffle the data?\n",
        "Solution: otherwise, suppose we train a NN classifier and have\n",
        "two classes — A and B, and that all samples of one class come\n",
        "before the other class. Not shuffling the data will make the\n",
        "weights converge to a wrong value.\n",
        "122. Describe some hyperparameters for transfer learning.\n",
        "Solution: How many layers to keep, how many layers to add,\n",
        "how many to freeze.\n",
        "123. Is dropout used on the test set?\n",
        "Solution: No! only in the train set. Dropout is a regularization\n",
        "technique that is applied in the training process.\n",
        "124. Explain why dropout in a neural network acts as a\n",
        "regularizer.\n",
        "Solution: There are several (related) explanations to why\n",
        "dropout works. It can be seen as a form of model averaging — at\n",
        "each step we “turn off” a part of the model and average the\n",
        "models we get. It also adds noise, which naturally has a\n",
        "regularizing effect. It also leads to more sparsity of the weights\n",
        "and essentially prevents co-adaptation of neurons in the\n",
        "network.\n",
        "125. Give examples in which a many-to-one RNN architecture\n",
        "is appropriate.\n",
        "Solution: A few examples are: sentiment analysis, gender\n",
        "recognition from speech, .\n",
        "126. When can’t we use BiLSTM? Explain what assumption has\n",
        "to be made.\n",
        "Solution: in any bi-directional model, we assume that we have\n",
        "access to the next elements of the sequence in a given “time”.\n",
        "This is the case for text data (i.e sentiment analysis, translation\n",
        "etc.), but not the case for time-series data.\n",
        "127. True/false: adding L2 regularization to a RNN can help\n",
        "with the vanishing gradient problem.\n",
        "Solution: false! Adding L2 regularization will shrink the\n",
        "weights towards zero, which can actually make the vanishing\n",
        "gradients worse in some cases.\n",
        "128. Suppose the training error/cost is high and that the\n",
        "validation cost/error is almost equal to it. What does it mean?\n",
        "What should be done?\n",
        "Solution: this indicates underfitting. One can add more\n",
        "parameters, increase the complexity of the model, or lower the\n",
        "regularization.\n",
        "129. Describe how L2 regularization can be explained as a sort\n",
        "of a weight decay.\n",
        "Solution: Suppose our cost function is C(w), and that we add a\n",
        "penalization c|w|2 . When using gradient descent, the iterations\n",
        "will look like\n",
        "w = w -grad(C)(w) — 2cw = (1–2c)w —\n",
        "grad(C)(w)\n",
        "In this equation, the weight is multiplied by a factor < 1.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0N_-7YKG1QR"
      },
      "source": [
        "# Import the packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPasDSydGS7g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dropout, Dense\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXDN48fdI13d"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoK5JPMfG4jS"
      },
      "outputs": [],
      "source": [
        "tk=Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcDudO6bHc4n"
      },
      "outputs": [],
      "source": [
        "tk.fit_on_texts([questions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6RVzID1IoT0",
        "outputId": "1f222052-aa91-4310-b13b-8765c680613e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'a': 2,\n",
              " 'is': 3,\n",
              " 'to': 4,\n",
              " 'of': 5,\n",
              " 'and': 6,\n",
              " 'in': 7,\n",
              " 'it': 8,\n",
              " 'learning': 9,\n",
              " 'are': 10,\n",
              " 'network': 11,\n",
              " 'what': 12,\n",
              " 'data': 13,\n",
              " 'function': 14,\n",
              " 'we': 15,\n",
              " 'neural': 16,\n",
              " 'for': 17,\n",
              " 'deep': 18,\n",
              " 'layer': 19,\n",
              " 'as': 20,\n",
              " 'this': 21,\n",
              " 'can': 22,\n",
              " 'you': 23,\n",
              " 'that': 24,\n",
              " 'o': 25,\n",
              " 'an': 26,\n",
              " 'model': 27,\n",
              " 'with': 28,\n",
              " 'or': 29,\n",
              " 'gradient': 30,\n",
              " 'be': 31,\n",
              " 'used': 32,\n",
              " 'which': 33,\n",
              " 'input': 34,\n",
              " 'output': 35,\n",
              " 'activation': 36,\n",
              " 'by': 37,\n",
              " 'weights': 38,\n",
              " '•': 39,\n",
              " 'on': 40,\n",
              " 'networks': 41,\n",
              " 'have': 42,\n",
              " 'one': 43,\n",
              " 'all': 44,\n",
              " 'how': 45,\n",
              " 'at': 46,\n",
              " 'layers': 47,\n",
              " 'each': 48,\n",
              " 'training': 49,\n",
              " '1': 50,\n",
              " 'if': 51,\n",
              " 'not': 52,\n",
              " 'image': 53,\n",
              " 'same': 54,\n",
              " '0': 55,\n",
              " 'batch': 56,\n",
              " 'when': 57,\n",
              " 'descent': 58,\n",
              " 'will': 59,\n",
              " 'solution': 60,\n",
              " 'hidden': 61,\n",
              " 'from': 62,\n",
              " 'relu': 63,\n",
              " 'use': 64,\n",
              " 'there': 65,\n",
              " 'using': 66,\n",
              " 'algorithm': 67,\n",
              " 'value': 68,\n",
              " 'would': 69,\n",
              " 'here': 70,\n",
              " 'matrix': 71,\n",
              " 'like': 72,\n",
              " 'has': 73,\n",
              " 'more': 74,\n",
              " 'into': 75,\n",
              " 'x': 76,\n",
              " 'why': 77,\n",
              " 'machine': 78,\n",
              " 'two': 79,\n",
              " 'functions': 80,\n",
              " 'between': 81,\n",
              " 'some': 82,\n",
              " '2': 83,\n",
              " 'neurons': 84,\n",
              " 'rnn': 85,\n",
              " 'backpropagation': 86,\n",
              " 'error': 87,\n",
              " 'cost': 88,\n",
              " 'do': 89,\n",
              " 'step': 90,\n",
              " 'bias': 91,\n",
              " 'time': 92,\n",
              " 'learn': 93,\n",
              " 'called': 94,\n",
              " 'cnn': 95,\n",
              " 'also': 96,\n",
              " 'its': 97,\n",
              " 'only': 98,\n",
              " 'most': 99,\n",
              " 'linear': 100,\n",
              " 'models': 101,\n",
              " 'parameters': 102,\n",
              " 'tf': 103,\n",
              " 'explain': 104,\n",
              " '–': 105,\n",
              " 'other': 106,\n",
              " 'information': 107,\n",
              " 'these': 108,\n",
              " 'just': 109,\n",
              " 'recurrent': 110,\n",
              " 'so': 111,\n",
              " 'well': 112,\n",
              " 'then': 113,\n",
              " '3': 114,\n",
              " 'different': 115,\n",
              " 'inputs': 116,\n",
              " 'such': 117,\n",
              " 'they': 118,\n",
              " 'than': 119,\n",
              " 'example': 120,\n",
              " '—': 121,\n",
              " 'better': 122,\n",
              " 'but': 123,\n",
              " 'process': 124,\n",
              " 'your': 125,\n",
              " 'set': 126,\n",
              " 'too': 127,\n",
              " 'weight': 128,\n",
              " 'images': 129,\n",
              " 'need': 130,\n",
              " 'stochastic': 131,\n",
              " 'no': 132,\n",
              " 'softmax': 133,\n",
              " 'does': 134,\n",
              " 'gradients': 135,\n",
              " 'while': 136,\n",
              " 'small': 137,\n",
              " 'wine': 138,\n",
              " 'make': 139,\n",
              " 'question': 140,\n",
              " 'propagation': 141,\n",
              " 'our': 142,\n",
              " 'much': 143,\n",
              " 'where': 144,\n",
              " 'should': 145,\n",
              " 'understand': 146,\n",
              " 'updates': 147,\n",
              " 'zero': 148,\n",
              " 'give': 149,\n",
              " 'entire': 150,\n",
              " 'them': 151,\n",
              " 'dimensions': 152,\n",
              " 'lstm': 153,\n",
              " 'vanishing': 154,\n",
              " 'classification': 155,\n",
              " 'mini': 156,\n",
              " 'complex': 157,\n",
              " 'train': 158,\n",
              " 'patterns': 159,\n",
              " 'perceptron': 160,\n",
              " 'single': 161,\n",
              " 'normalization': 162,\n",
              " 'processing': 163,\n",
              " 'boltzmann': 164,\n",
              " 'any': 165,\n",
              " 'many': 166,\n",
              " 'dataset': 167,\n",
              " 'because': 168,\n",
              " 'might': 169,\n",
              " 'real': 170,\n",
              " 'work': 171,\n",
              " 'size': 172,\n",
              " 'tensorflow': 173,\n",
              " 'us': 174,\n",
              " 'representation': 175,\n",
              " 'simple': 176,\n",
              " 'artificial': 177,\n",
              " 'large': 178,\n",
              " 'features': 179,\n",
              " '”': 180,\n",
              " 'uses': 181,\n",
              " 'nodes': 182,\n",
              " 'makes': 183,\n",
              " 'neuron': 184,\n",
              " 'sigmoid': 185,\n",
              " 'difference': 186,\n",
              " 'number': 187,\n",
              " 'high': 188,\n",
              " 'point': 189,\n",
              " 'every': 190,\n",
              " 'mean': 191,\n",
              " 'both': 192,\n",
              " 'convolutional': 193,\n",
              " 'smaller': 194,\n",
              " 'filter': 195,\n",
              " 'steps': 196,\n",
              " 'exploding': 197,\n",
              " 'problem': 198,\n",
              " 'b': 199,\n",
              " 'result': 200,\n",
              " 'fake': 201,\n",
              " 'create': 202,\n",
              " 'i': 203,\n",
              " 'unsupervised': 204,\n",
              " 'autoencoder': 205,\n",
              " 'computation': 206,\n",
              " 'operations': 207,\n",
              " 'important': 208,\n",
              " 'multi': 209,\n",
              " 'node': 210,\n",
              " 'values': 211,\n",
              " 'basic': 212,\n",
              " 'performance': 213,\n",
              " 'dropout': 214,\n",
              " 'update': 215,\n",
              " 'specific': 216,\n",
              " 'accuracy': 217,\n",
              " 'perform': 218,\n",
              " 'since': 219,\n",
              " 'state': 220,\n",
              " 'part': 221,\n",
              " 'c': 222,\n",
              " 'elements': 223,\n",
              " 'graph': 224,\n",
              " 'w': 225,\n",
              " 'forward': 226,\n",
              " 'sequence': 227,\n",
              " 'about': 228,\n",
              " 'f': 229,\n",
              " 'end': 230,\n",
              " 'adding': 231,\n",
              " 'initialization': 232,\n",
              " 'feature': 233,\n",
              " 'etc': 234,\n",
              " 'binary': 235,\n",
              " 'supervised': 236,\n",
              " 'get': 237,\n",
              " 'particular': 238,\n",
              " 'convergence': 239,\n",
              " '5': 240,\n",
              " '7': 241,\n",
              " 'good': 242,\n",
              " 'during': 243,\n",
              " 'technique': 244,\n",
              " 'improve': 245,\n",
              " 'text': 246,\n",
              " 'unit': 247,\n",
              " 'rate': 248,\n",
              " 'very': 249,\n",
              " 'even': 250,\n",
              " 'out': 251,\n",
              " 'randomly': 252,\n",
              " 'overfitting': 253,\n",
              " 'underfitting': 254,\n",
              " 'new': 255,\n",
              " 'either': 256,\n",
              " 'operation': 257,\n",
              " 'over': 258,\n",
              " 'long': 259,\n",
              " 'term': 260,\n",
              " 'add': 261,\n",
              " 'represent': 262,\n",
              " 'shop': 263,\n",
              " 'case': 264,\n",
              " 'regression': 265,\n",
              " 'main': 266,\n",
              " 'calculate': 267,\n",
              " 'their': 268,\n",
              " 'helps': 269,\n",
              " 'next': 270,\n",
              " 'up': 271,\n",
              " 'activations': 272,\n",
              " 'regularization': 273,\n",
              " 'convolution': 274,\n",
              " 'padding': 275,\n",
              " 'possible': 276,\n",
              " 'advantages': 277,\n",
              " 'taking': 278,\n",
              " 'performs': 279,\n",
              " 'common': 280,\n",
              " 'three': 281,\n",
              " 'faster': 282,\n",
              " 'contains': 283,\n",
              " 'structure': 284,\n",
              " 'backward': 285,\n",
              " 'often': 286,\n",
              " 'range': 287,\n",
              " 'whether': 288,\n",
              " 'examples': 289,\n",
              " 'through': 290,\n",
              " 'minimize': 291,\n",
              " 'take': 292,\n",
              " 'e': 293,\n",
              " 'analysis': 294,\n",
              " 'problems': 295,\n",
              " 'outputs': 296,\n",
              " 'usually': 297,\n",
              " 'able': 298,\n",
              " 'several': 299,\n",
              " 'become': 300,\n",
              " 'known': 301,\n",
              " 'instead': 302,\n",
              " 'epoch': 303,\n",
              " 'iteration': 304,\n",
              " 'constant': 305,\n",
              " 'tensors': 306,\n",
              " 'suppose': 307,\n",
              " 'owner': 308,\n",
              " 'techniques': 309,\n",
              " 'above': 310,\n",
              " 'logistic': 311,\n",
              " 'best': 312,\n",
              " 'results': 313,\n",
              " 'include': 314,\n",
              " 'z': 315,\n",
              " 'order': 316,\n",
              " 'factors': 317,\n",
              " 'final': 318,\n",
              " 'thus': 319,\n",
              " 'rnns': 320,\n",
              " 'given': 321,\n",
              " 'n': 322,\n",
              " 'task': 323,\n",
              " 'few': 324,\n",
              " 'describe': 325,\n",
              " 'first': 326,\n",
              " 'algorithms': 327,\n",
              " 'instance': 328,\n",
              " 'takes': 329,\n",
              " 'made': 330,\n",
              " 'means': 331,\n",
              " 'help': 332,\n",
              " 'level': 333,\n",
              " 'sum': 334,\n",
              " 'tanh': 335,\n",
              " 'cannot': 336,\n",
              " 'previous': 337,\n",
              " 'g': 338,\n",
              " 'applications': 339,\n",
              " 'parameter': 340,\n",
              " 'before': 341,\n",
              " 'making': 342,\n",
              " 'converge': 343,\n",
              " 'learns': 344,\n",
              " 'noise': 345,\n",
              " 'target': 346,\n",
              " 'go': 347,\n",
              " 'non': 348,\n",
              " 'pooling': 349,\n",
              " 'short': 350,\n",
              " 'memory': 351,\n",
              " 'forget': 352,\n",
              " 'higher': 353,\n",
              " 'allow': 354,\n",
              " 'session': 355,\n",
              " 'form': 356,\n",
              " 'original': 357,\n",
              " 'ones': 358,\n",
              " 'discriminator': 359,\n",
              " 'points': 360,\n",
              " 'generate': 361,\n",
              " 'threshold': 362,\n",
              " 'however': 363,\n",
              " 'start': 364,\n",
              " 'now': 365,\n",
              " 'capture': 366,\n",
              " 'without': 367,\n",
              " 'solve': 368,\n",
              " 'explained': 369,\n",
              " 'decision': 370,\n",
              " 'changes': 371,\n",
              " 'biases': 372,\n",
              " 'back': 373,\n",
              " 'always': 374,\n",
              " 'loop': 375,\n",
              " 'sequential': 376,\n",
              " 'dimension': 377,\n",
              " 'computer': 378,\n",
              " 'issues': 379,\n",
              " 'was': 380,\n",
              " 'architecture': 381,\n",
              " 'project': 382,\n",
              " 'code': 383,\n",
              " 'general': 384,\n",
              " 'efficient': 385,\n",
              " 'following': 386,\n",
              " 'class': 387,\n",
              " 'cat': 388,\n",
              " 'way': 389,\n",
              " 'performing': 390,\n",
              " 'mlp': 391,\n",
              " 'classify': 392,\n",
              " 'nonlinear': 393,\n",
              " 'based': 394,\n",
              " 'being': 395,\n",
              " 'cases': 396,\n",
              " 'connected': 397,\n",
              " 'across': 398,\n",
              " 'referred': 399,\n",
              " 'evaluate': 400,\n",
              " 'compute': 401,\n",
              " 'reduce': 402,\n",
              " 'feedforward': 403,\n",
              " 'sentiment': 404,\n",
              " 'equal': 405,\n",
              " 'rectified': 406,\n",
              " 'gives': 407,\n",
              " 'low': 408,\n",
              " 'may': 409,\n",
              " 'details': 410,\n",
              " 'shape': 411,\n",
              " 'saw': 412,\n",
              " 'less': 413,\n",
              " 'having': 414,\n",
              " 'initialized': 415,\n",
              " 'initialize': 416,\n",
              " 'similar': 417,\n",
              " 'giving': 418,\n",
              " 'four': 419,\n",
              " 'negative': 420,\n",
              " 'map': 421,\n",
              " 'special': 422,\n",
              " 'represents': 423,\n",
              " 'run': 424,\n",
              " 'compared': 425,\n",
              " 'mathematical': 426,\n",
              " 'arrays': 427,\n",
              " 'float32': 428,\n",
              " 'variable': 429,\n",
              " 'outside': 430,\n",
              " 'later': 431,\n",
              " 'sess': 432,\n",
              " 'edges': 433,\n",
              " 'adversarial': 434,\n",
              " 'authentic': 435,\n",
              " 'sure': 436,\n",
              " 'tries': 437,\n",
              " 'encoder': 438,\n",
              " 'probability': 439,\n",
              " 'essentially': 440,\n",
              " 'equation': 441,\n",
              " 'terms': 442,\n",
              " 'derivative': 443,\n",
              " 'further': 444,\n",
              " 'tasks': 445,\n",
              " 'lower': 446,\n",
              " 'ways': 447,\n",
              " 'interview': 448,\n",
              " 'l2': 449,\n",
              " 'asked': 450,\n",
              " 'key': 451,\n",
              " 'basically': 452,\n",
              " 'pixel': 453,\n",
              " 'multiplication': 454,\n",
              " 'nature': 455,\n",
              " 'vision': 456,\n",
              " 'leads': 457,\n",
              " 'gate': 458,\n",
              " 'second': 459,\n",
              " 'recognition': 460,\n",
              " 'machines': 461,\n",
              " 'provide': 462,\n",
              " 'procedure': 463,\n",
              " 'concerning': 464,\n",
              " 'available': 465,\n",
              " 'responsible': 466,\n",
              " 'swish': 467,\n",
              " 'decoder': 468,\n",
              " 'capacity': 469,\n",
              " 'filters': 470,\n",
              " 'perceptrons': 471,\n",
              " 'd': 472,\n",
              " 'involves': 473,\n",
              " 'simpler': 474,\n",
              " 'classes': 475,\n",
              " 'coming': 476,\n",
              " 'producing': 477,\n",
              " 'method': 478,\n",
              " '4': 479,\n",
              " 'it’s': 480,\n",
              " 'comes': 481,\n",
              " 'fit': 482,\n",
              " 'decides': 483,\n",
              " 'aim': 484,\n",
              " 'find': 485,\n",
              " 'minima': 486,\n",
              " 'determines': 487,\n",
              " 'direction': 488,\n",
              " '10': 489,\n",
              " 'feedback': 490,\n",
              " 'series': 491,\n",
              " 'otherwise': 492,\n",
              " 'hyperparameters': 493,\n",
              " 'once': 494,\n",
              " 'trained': 495,\n",
              " 'units': 496,\n",
              " 'slowly': 497,\n",
              " 'minimum': 498,\n",
              " 'behavior': 499,\n",
              " 'iterations': 500,\n",
              " 'standard': 501,\n",
              " 'occurs': 502,\n",
              " 'likely': 503,\n",
              " 'kind': 504,\n",
              " 'poor': 505,\n",
              " 'validation': 506,\n",
              " 'initializing': 507,\n",
              " 'close': 508,\n",
              " 'down': 509,\n",
              " 'sampling': 510,\n",
              " 'dimensionality': 511,\n",
              " 'capable': 512,\n",
              " 'dependencies': 513,\n",
              " 'slope': 514,\n",
              " 'difficult': 515,\n",
              " 'lead': 516,\n",
              " 'times': 517,\n",
              " 'put': 518,\n",
              " 'refers': 519,\n",
              " '50': 520,\n",
              " 'provides': 521,\n",
              " 'python': 522,\n",
              " 'tensor': 523,\n",
              " 'object': 524,\n",
              " 'fed': 525,\n",
              " 'programming': 526,\n",
              " 'change': 527,\n",
              " 'define': 528,\n",
              " 'command': 529,\n",
              " 'print': 530,\n",
              " 'variables': 531,\n",
              " 'placeholder': 532,\n",
              " 'computational': 533,\n",
              " 'generative': 534,\n",
              " 'forger': 535,\n",
              " 'vector': 536,\n",
              " 'acts': 537,\n",
              " 'generator': 538,\n",
              " 'works': 539,\n",
              " 'latent': 540,\n",
              " 'space': 541,\n",
              " 'bagging': 542,\n",
              " 'boosting': 543,\n",
              " 'know': 544,\n",
              " 'exactly': 545,\n",
              " 'note': 546,\n",
              " 'explanation': 547,\n",
              " 'according': 548,\n",
              " 'apply': 549,\n",
              " 'y': 550,\n",
              " 'nothing': 551,\n",
              " 'questions': 552,\n",
              " 'below': 553,\n",
              " 'list': 554,\n",
              " 'word': 555,\n",
              " 'understanding': 556,\n",
              " 'autoencoders': 557,\n",
              " 'article': 558,\n",
              " 'play': 559,\n",
              " 'affect': 560,\n",
              " 'interviewer': 561,\n",
              " 'move': 562,\n",
              " 'dataframe': 563,\n",
              " '100': 564,\n",
              " 'stored': 565,\n",
              " 'pytorch': 566,\n",
              " 'read': 567,\n",
              " 'component': 568,\n",
              " 'fundamentals': 569,\n",
              " 'normalize': 570,\n",
              " 'optimization': 571,\n",
              " 'projects': 572,\n",
              " 'follow': 573,\n",
              " 'parts': 574,\n",
              " 'detect': 575,\n",
              " 'another': 576,\n",
              " 'cnns': 577,\n",
              " 'sentence': 578,\n",
              " 'least': 579,\n",
              " 'formula': 580,\n",
              " 'towards': 581,\n",
              " 'larger': 582,\n",
              " 'transfer': 583,\n",
              " 'pretrained': 584,\n",
              " 'earlier': 585,\n",
              " 'lstms': 586,\n",
              " 'gates': 587,\n",
              " 'see': 588,\n",
              " 'grus': 589,\n",
              " 'attention': 590,\n",
              " 'natural': 591,\n",
              " 'language': 592,\n",
              " 'purpose': 593,\n",
              " 'execute': 594,\n",
              " 'transformers': 595,\n",
              " 'tools': 596,\n",
              " 'frameworks': 597,\n",
              " 'had': 598,\n",
              " 'ai': 599,\n",
              " 'stands': 600,\n",
              " 'translation': 601,\n",
              " 'shallow': 602,\n",
              " 'deeper': 603,\n",
              " 'multilayer': 604,\n",
              " 'independent': 605,\n",
              " 'scientists': 606,\n",
              " 'complexity': 607,\n",
              " 'belief': 608,\n",
              " 'introduce': 609,\n",
              " 'combination': 610,\n",
              " 'types': 611,\n",
              " 'leaky': 612,\n",
              " 'transformed': 613,\n",
              " 'probabilities': 614,\n",
              " 'automatically': 615,\n",
              " 'consists': 616,\n",
              " 'internal': 617,\n",
              " 'present': 618,\n",
              " 'optimize': 619,\n",
              " 'keep': 620,\n",
              " 'compressed': 621,\n",
              " 'necessary': 622,\n",
              " 'assumption': 623,\n",
              " 'idea': 624,\n",
              " 'regularizing': 625,\n",
              " 'effect': 626,\n",
              " 'dog': 627,\n",
              " 'humans': 628,\n",
              " 'inspired': 629,\n",
              " 'consist': 630,\n",
              " 'place': 631,\n",
              " 'various': 632,\n",
              " 'gan': 633,\n",
              " 'separable': 634,\n",
              " 'upon': 635,\n",
              " 'together': 636,\n",
              " 'propagates': 637,\n",
              " 'accurately': 638,\n",
              " 'rescale': 639,\n",
              " 'version': 640,\n",
              " 'visible': 641,\n",
              " 'net': 642,\n",
              " 'decisions': 643,\n",
              " 'off': 644,\n",
              " '6': 645,\n",
              " 'role': 646,\n",
              " 'weighted': 647,\n",
              " 'model’s': 648,\n",
              " 'optimal': 649,\n",
              " 'local': 650,\n",
              " 'signals': 651,\n",
              " 'current': 652,\n",
              " 'predicting': 653,\n",
              " 'widely': 654,\n",
              " 'working': 655,\n",
              " 'hyperparameter': 656,\n",
              " 'whose': 657,\n",
              " 'happen': 658,\n",
              " 'minimal': 659,\n",
              " 'due': 660,\n",
              " 'dropping': 661,\n",
              " 'typically': 662,\n",
              " '20': 663,\n",
              " 'needed': 664,\n",
              " 'normalizing': 665,\n",
              " 'deviation': 666,\n",
              " '16': 667,\n",
              " 'computes': 668,\n",
              " 'volume': 669,\n",
              " 'huge': 670,\n",
              " 'sample': 671,\n",
              " 'combat': 672,\n",
              " 'occur': 673,\n",
              " 'looking': 674,\n",
              " 'trucks': 675,\n",
              " 'recognizes': 676,\n",
              " 'truck': 677,\n",
              " 'happens': 678,\n",
              " 'methods': 679,\n",
              " 'assign': 680,\n",
              " 'assigned': 681,\n",
              " 'computations': 682,\n",
              " 'creating': 683,\n",
              " 'picture': 684,\n",
              " 'brings': 685,\n",
              " 'linearity': 686,\n",
              " 'converts': 687,\n",
              " 'pixels': 688,\n",
              " 'reduces': 689,\n",
              " 'spatial': 690,\n",
              " 'sliding': 691,\n",
              " 'default': 692,\n",
              " 'remember': 693,\n",
              " 'everything': 694,\n",
              " 'batches': 695,\n",
              " '000': 696,\n",
              " 'divided': 697,\n",
              " 'preferred': 698,\n",
              " 'easier': 699,\n",
              " 'keras': 700,\n",
              " 'computing': 701,\n",
              " 'constants': 702,\n",
              " 'dtype': 703,\n",
              " 'feed': 704,\n",
              " 'operates': 705,\n",
              " 'dealers': 706,\n",
              " 'sell': 707,\n",
              " 'try': 708,\n",
              " 'goal': 709,\n",
              " 'gets': 710,\n",
              " 'appearance': 711,\n",
              " 'auto': 712,\n",
              " 'reduction': 713,\n",
              " '30': 714,\n",
              " 'multiple': 715,\n",
              " 'call': 716,\n",
              " 'test': 717,\n",
              " 'wrong': 718,\n",
              " 'what’s': 719,\n",
              " 's': 720,\n",
              " 'built': 721,\n",
              " 'expected': 722,\n",
              " 'performed': 723,\n",
              " 'becomes': 724,\n",
              " 'yes': 725,\n",
              " 'evident': 726,\n",
              " 'simplest': 727,\n",
              " 'signal': 728,\n",
              " 'therefore': 729,\n",
              " 'initial': 730,\n",
              " 'covered': 731,\n",
              " 'hand': 732,\n",
              " 'embeddings': 733,\n",
              " 'words': 734,\n",
              " 'great': 735,\n",
              " 'introduction': 736,\n",
              " 'want': 737,\n",
              " 'cricket': 738,\n",
              " 'match': 739,\n",
              " 'friends': 740,\n",
              " 'game': 741,\n",
              " 'equipment': 742,\n",
              " 'temperature': 743,\n",
              " 'bat': 744,\n",
              " 'share': 745,\n",
              " 'factor': 746,\n",
              " 'provided': 747,\n",
              " 'calculating': 748,\n",
              " 'rule': 749,\n",
              " 'respect': 750,\n",
              " 'actual': 751,\n",
              " 'derivatives': 752,\n",
              " 'structures': 753,\n",
              " 'right': 754,\n",
              " 'complicated': 755,\n",
              " 'graphs': 756,\n",
              " 'ordered': 757,\n",
              " 'rows': 758,\n",
              " 'columns': 759,\n",
              " 'denoting': 760,\n",
              " 'datapoint': 761,\n",
              " 'students': 762,\n",
              " 'basis': 763,\n",
              " 'multidimensional': 764,\n",
              " 'improving': 765,\n",
              " 'reducing': 766,\n",
              " 'guide': 767,\n",
              " '300': 768,\n",
              " 'dividing': 769,\n",
              " 'matrices': 770,\n",
              " 'dealing': 771,\n",
              " 'addition': 772,\n",
              " 'mechanism': 773,\n",
              " 'ann': 774,\n",
              " 'around': 775,\n",
              " '2p': 776,\n",
              " 'valid': 777,\n",
              " 'after': 778,\n",
              " 'cause': 779,\n",
              " 'unstable': 780,\n",
              " 'almost': 781,\n",
              " 'specifically': 782,\n",
              " 'included': 783,\n",
              " 'intermediate': 784,\n",
              " 'reason': 785,\n",
              " 'milestones': 786,\n",
              " 'nlp': 787,\n",
              " 'popular': 788,\n",
              " 'earn': 789,\n",
              " 'additional': 790,\n",
              " 'considered': 791,\n",
              " 'gru': 792,\n",
              " 'detail': 793,\n",
              " 'transformer': 794,\n",
              " 'power': 795,\n",
              " 'mapping': 796,\n",
              " 'latest': 797,\n",
              " 'advanced': 798,\n",
              " 'updated': 799,\n",
              " 'explanations': 800,\n",
              " 'framework': 801,\n",
              " 'preprocessing': 802,\n",
              " 'sophisticated': 803,\n",
              " 'detection': 804,\n",
              " 'speech': 805,\n",
              " 'enables': 806,\n",
              " 'human': 807,\n",
              " 'enable': 808,\n",
              " 'procedures': 809,\n",
              " 'system': 810,\n",
              " 'desired': 811,\n",
              " 'pattern': 812,\n",
              " 'automatic': 813,\n",
              " 'handwriting': 814,\n",
              " 'generation': 815,\n",
              " 'allows': 816,\n",
              " 'calculated': 817,\n",
              " 'fourier': 818,\n",
              " 'transform': 819,\n",
              " 'analyzing': 820,\n",
              " 'categories': 821,\n",
              " 'autonomous': 822,\n",
              " 'free': 823,\n",
              " 'significant': 824,\n",
              " 'revolution': 825,\n",
              " 'field': 826,\n",
              " 'concept': 827,\n",
              " 'day': 828,\n",
              " 'disadvantages': 829,\n",
              " 'sets': 830,\n",
              " '65': 831,\n",
              " 'essential': 832,\n",
              " 'prevents': 833,\n",
              " 'starting': 834,\n",
              " 'self': 835,\n",
              " 'send': 836,\n",
              " 'activated': 837,\n",
              " '1990s': 838,\n",
              " 'generally': 839,\n",
              " 'easily': 840,\n",
              " 'representing': 841,\n",
              " \"it's\": 842,\n",
              " 'averaging': 843,\n",
              " 'hopfield': 844,\n",
              " 'type': 845,\n",
              " 'adaptive': 846,\n",
              " 'approximate': 847,\n",
              " 'amount': 848,\n",
              " 'whole': 849,\n",
              " 'moving': 850,\n",
              " 'θ': 851,\n",
              " 'samples': 852,\n",
              " 'improves': 853,\n",
              " 'avoid': 854,\n",
              " 'element': 855,\n",
              " 'wise': 856,\n",
              " 'product': 857,\n",
              " 'corresponding': 858,\n",
              " 'complete': 859,\n",
              " 'connections': 860,\n",
              " 'seen': 861,\n",
              " 'sequences': 862,\n",
              " 'precise': 863,\n",
              " 'applied': 864,\n",
              " 'tend': 865,\n",
              " 'compare': 866,\n",
              " 'prediction': 867,\n",
              " 'accumulate': 868,\n",
              " 'useful': 869,\n",
              " '98': 870,\n",
              " 'reduced': 871,\n",
              " 'raw': 872,\n",
              " 'five': 873,\n",
              " 'half': 874,\n",
              " 'developing': 875,\n",
              " 'rather': 876,\n",
              " 'dnn': 877,\n",
              " 'look': 878,\n",
              " 'visualize': 879,\n",
              " '2nd': 880,\n",
              " 'nn': 881,\n",
              " 'behind': 882,\n",
              " 'adam': 883,\n",
              " 'momentum': 884,\n",
              " 'augmentation': 885,\n",
              " 'increase': 886,\n",
              " 'gans': 887,\n",
              " 'batchnorm': 888,\n",
              " 'm': 889,\n",
              " 'shuffle': 890,\n",
              " 'shuffling': 891,\n",
              " 'false': 892,\n",
              " 'grad': 893,\n",
              " 'volumes': 894,\n",
              " 'structured': 895,\n",
              " 'unstructured': 896,\n",
              " 'extract': 897,\n",
              " 'distinguishing': 898,\n",
              " 'replicate': 899,\n",
              " 'brains': 900,\n",
              " 'fire': 901,\n",
              " 'extraction': 902,\n",
              " 'adjustments': 903,\n",
              " 'sheet': 904,\n",
              " '“nodes': 905,\n",
              " 'mlps': 906,\n",
              " 'except': 907,\n",
              " 'added': 908,\n",
              " '“backpropagation': 909,\n",
              " 'calculates': 910,\n",
              " 'came': 911,\n",
              " 'adjusts': 912,\n",
              " 'standardizing': 913,\n",
              " 'reforming': 914,\n",
              " '“data': 915,\n",
              " 'pre': 916,\n",
              " 'eliminate': 917,\n",
              " 'redundancy': 918,\n",
              " 'formats': 919,\n",
              " 'achieving': 920,\n",
              " 'resembling': 921,\n",
              " 'simplified': 922,\n",
              " 'fired': 923,\n",
              " 'accepts': 924,\n",
              " '“loss”': 925,\n",
              " '“error': 926,\n",
              " 'measure': 927,\n",
              " 'push': 928,\n",
              " '8': 929,\n",
              " 'global': 930,\n",
              " '9': 931,\n",
              " 'backpropagates': 932,\n",
              " 'travel': 933,\n",
              " 'loops': 934,\n",
              " 'considers': 935,\n",
              " 'memorize': 936,\n",
              " '11': 937,\n",
              " 'mining': 938,\n",
              " 'captioning': 939,\n",
              " 'address': 940,\n",
              " 'prices': 941,\n",
              " 'stocks': 942,\n",
              " 'month': 943,\n",
              " 'quarter': 944,\n",
              " '12': 945,\n",
              " 'generates': 946,\n",
              " 'divides': 947,\n",
              " 'total': 948,\n",
              " 'positive': 949,\n",
              " 'zeros': 950,\n",
              " '13': 951,\n",
              " 'you’re': 952,\n",
              " 'formatted': 953,\n",
              " 'correctly': 954,\n",
              " 'begins': 955,\n",
              " 'epochs': 956,\n",
              " '14': 957,\n",
              " 'progress': 958,\n",
              " 'reaching': 959,\n",
              " 'causes': 960,\n",
              " 'undesirable': 961,\n",
              " 'divergent': 962,\n",
              " 'loss': 963,\n",
              " 'drastic': 964,\n",
              " 'fail': 965,\n",
              " 'diverge': 966,\n",
              " 'chaotic': 967,\n",
              " '15': 968,\n",
              " 'prevent': 969,\n",
              " 'percent': 970,\n",
              " 'doubles': 971,\n",
              " 'stability': 972,\n",
              " 'converges': 973,\n",
              " 'frequently': 974,\n",
              " '17': 975,\n",
              " 'degree': 976,\n",
              " 'adversely': 977,\n",
              " 'impacts': 978,\n",
              " 'execution': 979,\n",
              " 'flexibility': 980,\n",
              " 'cars': 981,\n",
              " 'box': 982,\n",
              " 'notice': 983,\n",
              " 'flatbed': 984,\n",
              " \"there's\": 985,\n",
              " 'world': 986,\n",
              " 'alludes': 987,\n",
              " 'neither': 988,\n",
              " 'nor': 989,\n",
              " 'generalize': 990,\n",
              " 'incorrect': 991,\n",
              " 'resample': 992,\n",
              " 'estimate': 993,\n",
              " 'k': 994,\n",
              " 'fold': 995,\n",
              " 'cross': 996,\n",
              " '18': 997,\n",
              " 'useless': 998,\n",
              " 'commonly': 999,\n",
              " '19': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tk.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhA3iJUzPohY",
        "outputId": "8ca89e89-0af0-4375-91c6-53fbc92007aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1774"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "total_words=len(tk.word_index)\n",
        "total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HrhQJRlI0R8"
      },
      "outputs": [],
      "source": [
        "# for question in questions.split('\\n'):\n",
        "#   #print(question)\n",
        "#   print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWWj-20hJTCr"
      },
      "outputs": [],
      "source": [
        "# removes the numbers and convert to seqences\n",
        "input_questions=[]\n",
        "for question in questions.split('\\n'):\n",
        "    question_corrected = re.sub(r'[^a-zA-Z\\s]', '', question).lower()\n",
        "    tokenized_question=tk.texts_to_sequences([question_corrected])[0]\n",
        "    for i in range(1,len(tokenized_question)):\n",
        "     input_questions.append(tokenized_question[:i+1])#n_gram\n",
        "    #print(tk.texts_to_sequences([alphabets_only])[0])\n",
        "    # print(alphabets_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dggQULHyKQLN",
        "outputId": "beeaab60-439f-4f5a-e2a1-bbedea06ce06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 3],\n",
              " [12, 3, 18],\n",
              " [12, 3, 18, 9],\n",
              " [18, 9],\n",
              " [18, 9, 473],\n",
              " [18, 9, 473, 278],\n",
              " [18, 9, 473, 278, 178],\n",
              " [18, 9, 473, 278, 178, 894],\n",
              " [18, 9, 473, 278, 178, 894, 5],\n",
              " [18, 9, 473, 278, 178, 894, 5, 895],\n",
              " [18, 9, 473, 278, 178, 894, 5, 895, 29],\n",
              " [18, 9, 473, 278, 178, 894, 5, 895, 29, 896],\n",
              " [18, 9, 473, 278, 178, 894, 5, 895, 29, 896, 13],\n",
              " [18, 9, 473, 278, 178, 894, 5, 895, 29, 896, 13, 6],\n",
              " [66, 157],\n",
              " [66, 157, 327],\n",
              " [66, 157, 327, 4],\n",
              " [66, 157, 327, 4, 158],\n",
              " [66, 157, 327, 4, 158, 16],\n",
              " [66, 157, 327, 4, 158, 16, 41],\n",
              " [66, 157, 327, 4, 158, 16, 41, 8],\n",
              " [66, 157, 327, 4, 158, 16, 41, 8, 279],\n",
              " [66, 157, 327, 4, 158, 16, 41, 8, 279, 157],\n",
              " [66, 157, 327, 4, 158, 16, 41, 8, 279, 157, 207],\n",
              " [4, 897],\n",
              " [4, 897, 61],\n",
              " [4, 897, 61, 159],\n",
              " [4, 897, 61, 159, 6],\n",
              " [4, 897, 61, 159, 6, 179],\n",
              " [4, 897, 61, 159, 6, 179, 17],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328, 898],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328, 898, 1],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328, 898, 1, 53],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328, 898, 1, 53, 5],\n",
              " [4, 897, 61, 159, 6, 179, 17, 328, 898, 1, 53, 5, 2],\n",
              " [388, 62],\n",
              " [388, 62, 24],\n",
              " [388, 62, 24, 5],\n",
              " [388, 62, 24, 5, 2],\n",
              " [388, 62, 24, 5, 2, 627],\n",
              " [12, 3],\n",
              " [12, 3, 2],\n",
              " [12, 3, 2, 16],\n",
              " [12, 3, 2, 16, 11],\n",
              " [16, 41],\n",
              " [16, 41, 899],\n",
              " [16, 41, 899, 1],\n",
              " [16, 41, 899, 1, 389],\n",
              " [16, 41, 899, 1, 389, 628],\n",
              " [16, 41, 899, 1, 389, 628, 93],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37, 45],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37, 45, 1],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37, 45, 1, 84],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37, 45, 1, 84, 7],\n",
              " [16, 41, 899, 1, 389, 628, 93, 629, 37, 45, 1, 84, 7, 142],\n",
              " [900, 901],\n",
              " [900, 901, 98],\n",
              " [900, 901, 98, 143],\n",
              " [900, 901, 98, 143, 474],\n",
              " [1, 99],\n",
              " [1, 99, 280],\n",
              " [1, 99, 280, 16],\n",
              " [1, 99, 280, 16, 41],\n",
              " [1, 99, 280, 16, 41, 630],\n",
              " [1, 99, 280, 16, 41, 630, 5],\n",
              " [1, 99, 280, 16, 41, 630, 5, 281],\n",
              " [1, 99, 280, 16, 41, 630, 5, 281, 11],\n",
              " [1, 99, 280, 16, 41, 630, 5, 281, 11, 47],\n",
              " [26, 34],\n",
              " [26, 34, 19],\n",
              " [2, 61],\n",
              " [2, 61, 19],\n",
              " [2, 61, 19, 21],\n",
              " [2, 61, 19, 21, 3],\n",
              " [2, 61, 19, 21, 3, 1],\n",
              " [2, 61, 19, 21, 3, 1, 99],\n",
              " [2, 61, 19, 21, 3, 1, 99, 208],\n",
              " [2, 61, 19, 21, 3, 1, 99, 208, 19],\n",
              " [2, 61, 19, 21, 3, 1, 99, 208, 19, 144],\n",
              " [2, 61, 19, 21, 3, 1, 99, 208, 19, 144, 233],\n",
              " [2, 61, 19, 21, 3, 1, 99, 208, 19, 144, 233, 902],\n",
              " [329, 631],\n",
              " [329, 631, 6],\n",
              " [329, 631, 6, 903],\n",
              " [329, 631, 6, 903, 10],\n",
              " [329, 631, 6, 903, 10, 330],\n",
              " [329, 631, 6, 903, 10, 330, 4],\n",
              " [329, 631, 6, 903, 10, 330, 4, 158],\n",
              " [329, 631, 6, 903, 10, 330, 4, 158, 282],\n",
              " [329, 631, 6, 903, 10, 330, 4, 158, 282, 6],\n",
              " [329, 631, 6, 903, 10, 330, 4, 158, 282, 6, 14],\n",
              " [329, 631, 6, 903, 10, 330, 4, 158, 282, 6, 14, 122],\n",
              " [26, 35],\n",
              " [26, 35, 19],\n",
              " [48, 904],\n",
              " [48, 904, 283],\n",
              " [48, 904, 283, 84],\n",
              " [48, 904, 283, 84, 94],\n",
              " [48, 904, 283, 84, 94, 182],\n",
              " [48, 904, 283, 84, 94, 182, 390],\n",
              " [48, 904, 283, 84, 94, 182, 390, 632],\n",
              " [48, 904, 283, 84, 94, 182, 390, 632, 207],\n",
              " [48, 904, 283, 84, 94, 182, 390, 632, 207, 16],\n",
              " [41, 10],\n",
              " [41, 10, 32],\n",
              " [41, 10, 32, 7],\n",
              " [41, 10, 32, 7, 18],\n",
              " [41, 10, 32, 7, 18, 9],\n",
              " [41, 10, 32, 7, 18, 9, 327],\n",
              " [41, 10, 32, 7, 18, 9, 327, 72],\n",
              " [41, 10, 32, 7, 18, 9, 327, 72, 95],\n",
              " [41, 10, 32, 7, 18, 9, 327, 72, 95, 85],\n",
              " [41, 10, 32, 7, 18, 9, 327, 72, 95, 85, 633],\n",
              " [41, 10, 32, 7, 18, 9, 327, 72, 95, 85, 633, 234],\n",
              " [12, 3],\n",
              " [12, 3, 2],\n",
              " [12, 3, 2, 604],\n",
              " [20, 7],\n",
              " [20, 7, 16],\n",
              " [20, 7, 16, 41],\n",
              " [20, 7, 16, 41, 906],\n",
              " [20, 7, 16, 41, 906, 42],\n",
              " [20, 7, 16, 41, 906, 42, 26],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61, 19],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61, 19, 6],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61, 19, 6, 26],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61, 19, 6, 26, 35],\n",
              " [20, 7, 16, 41, 906, 42, 26, 34, 19, 2, 61, 19, 6, 26, 35, 19],\n",
              " [8, 73],\n",
              " [8, 73, 1],\n",
              " [8, 73, 1, 54],\n",
              " [8, 73, 1, 54, 284],\n",
              " [8, 73, 1, 54, 284, 20],\n",
              " [8, 73, 1, 54, 284, 20, 2],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160, 28],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160, 28, 43],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160, 28, 43, 29],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160, 28, 43, 29, 74],\n",
              " [8, 73, 1, 54, 284, 20, 2, 161, 19, 160, 28, 43, 29, 74, 61],\n",
              " [47, 2],\n",
              " [47, 2, 161],\n",
              " [47, 2, 161, 19],\n",
              " [47, 2, 161, 19, 160],\n",
              " [47, 2, 161, 19, 160, 22],\n",
              " [47, 2, 161, 19, 160, 22, 392],\n",
              " [47, 2, 161, 19, 160, 22, 392, 98],\n",
              " [47, 2, 161, 19, 160, 22, 392, 98, 100],\n",
              " [47, 2, 161, 19, 160, 22, 392, 98, 100, 634],\n",
              " [47, 2, 161, 19, 160, 22, 392, 98, 100, 634, 475],\n",
              " [47, 2, 161, 19, 160, 22, 392, 98, 100, 634, 475, 28],\n",
              " [235, 35],\n",
              " [235, 35, 123],\n",
              " [235, 35, 123, 391],\n",
              " [235, 35, 123, 391, 22],\n",
              " [235, 35, 123, 391, 22, 392],\n",
              " [235, 35, 123, 391, 22, 392, 393],\n",
              " [235, 35, 123, 391, 22, 392, 393, 475],\n",
              " [907, 17],\n",
              " [907, 17, 1],\n",
              " [907, 17, 1, 34],\n",
              " [907, 17, 1, 34, 19],\n",
              " [907, 17, 1, 34, 19, 48],\n",
              " [907, 17, 1, 34, 19, 48, 210],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106, 47],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106, 47, 181],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106, 47, 181, 2],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106, 47, 181, 2, 393],\n",
              " [907, 17, 1, 34, 19, 48, 210, 7, 1, 106, 47, 181, 2, 393, 36],\n",
              " [14, 21],\n",
              " [14, 21, 331],\n",
              " [14, 21, 331, 1],\n",
              " [14, 21, 331, 1, 34],\n",
              " [14, 21, 331, 1, 34, 47],\n",
              " [14, 21, 331, 1, 34, 47, 1],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476, 7],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476, 7, 6],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476, 7, 6, 1],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476, 7, 6, 1, 36],\n",
              " [14, 21, 331, 1, 34, 47, 1, 13, 476, 7, 6, 1, 36, 14],\n",
              " [3, 394],\n",
              " [3, 394, 635],\n",
              " [3, 394, 635, 44],\n",
              " [3, 394, 635, 44, 182],\n",
              " [3, 394, 635, 44, 182, 6],\n",
              " [3, 394, 635, 44, 182, 6, 38],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395, 908],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395, 908, 636],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395, 908, 636, 477],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395, 908, 636, 477, 1],\n",
              " [3, 394, 635, 44, 182, 6, 38, 395, 908, 636, 477, 1, 35],\n",
              " [391, 181],\n",
              " [391, 181, 2],\n",
              " [391, 181, 2, 236],\n",
              " [391, 181, 2, 236, 9],\n",
              " [391, 181, 2, 236, 9, 478],\n",
              " [391, 181, 2, 236, 9, 478, 94],\n",
              " [391, 181, 2, 236, 9, 478, 94, 86],\n",
              " [391, 181, 2, 236, 9, 478, 94, 86, 7],\n",
              " [86, 1],\n",
              " [86, 1, 16],\n",
              " [86, 1, 16, 11],\n",
              " [86, 1, 16, 11, 910],\n",
              " [86, 1, 16, 11, 910, 1],\n",
              " [86, 1, 16, 11, 910, 1, 87],\n",
              " [86, 1, 16, 11, 910, 1, 87, 28],\n",
              " [86, 1, 16, 11, 910, 1, 87, 28, 1],\n",
              " [86, 1, 16, 11, 910, 1, 87, 28, 1, 332],\n",
              " [86, 1, 16, 11, 910, 1, 87, 28, 1, 332, 5],\n",
              " [86, 1, 16, 11, 910, 1, 87, 28, 1, 332, 5, 88],\n",
              " [14, 8],\n",
              " [14, 8, 637],\n",
              " [14, 8, 637, 21],\n",
              " [14, 8, 637, 21, 87],\n",
              " [14, 8, 637, 21, 87, 285],\n",
              " [14, 8, 637, 21, 87, 285, 62],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144, 8],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144, 8, 911],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144, 8, 911, 912],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144, 8, 911, 912, 1],\n",
              " [14, 8, 637, 21, 87, 285, 62, 144, 8, 911, 912, 1, 38],\n",
              " [4, 158],\n",
              " [4, 158, 1],\n",
              " [4, 158, 1, 27],\n",
              " [4, 158, 1, 27, 74],\n",
              " [4, 158, 1, 27, 74, 638],\n",
              " [12, 3],\n",
              " [12, 3, 13],\n",
              " [12, 3, 13, 162],\n",
              " [12, 3, 13, 162, 6],\n",
              " [12, 3, 13, 162, 6, 77],\n",
              " [12, 3, 13, 162, 6, 77, 89],\n",
              " [12, 3, 13, 162, 6, 77, 89, 15],\n",
              " [12, 3, 13, 162, 6, 77, 89, 15, 130],\n",
              " [12, 3, 13, 162, 6, 77, 89, 15, 130, 8],\n",
              " [1, 124],\n",
              " [1, 124, 5],\n",
              " [1, 124, 5, 913],\n",
              " [1, 124, 5, 913, 6],\n",
              " [1, 124, 5, 913, 6, 914],\n",
              " [1, 124, 5, 913, 6, 914, 13],\n",
              " [1, 124, 5, 913, 6, 914, 13, 3],\n",
              " [1, 124, 5, 913, 6, 914, 13, 3, 94],\n",
              " [1, 124, 5, 913, 6, 914, 13, 3, 94, 13],\n",
              " [1, 124, 5, 913, 6, 914, 13, 3, 94, 13, 162],\n",
              " [1, 124, 5, 913, 6, 914, 13, 3, 94, 13, 162, 97],\n",
              " [2, 802],\n",
              " [2, 802, 90],\n",
              " [2, 802, 90, 4],\n",
              " [2, 802, 90, 4, 917],\n",
              " [2, 802, 90, 4, 917, 13],\n",
              " [2, 802, 90, 4, 917, 13, 918],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286, 13],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286, 13, 481],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286, 13, 481, 7],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286, 13, 481, 7, 6],\n",
              " [2, 802, 90, 4, 917, 13, 918, 286, 13, 481, 7, 6, 23],\n",
              " [237, 1],\n",
              " [237, 1, 54],\n",
              " [237, 1, 54, 107],\n",
              " [237, 1, 54, 107, 7],\n",
              " [237, 1, 54, 107, 7, 115],\n",
              " [237, 1, 54, 107, 7, 115, 919],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7, 108],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7, 108, 396],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7, 108, 396, 23],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7, 108, 396, 23, 145],\n",
              " [237, 1, 54, 107, 7, 115, 919, 7, 108, 396, 23, 145, 639],\n",
              " [211, 4],\n",
              " [211, 4, 482],\n",
              " [211, 4, 482, 75],\n",
              " [211, 4, 482, 75, 2],\n",
              " [211, 4, 482, 75, 2, 238],\n",
              " [211, 4, 482, 75, 2, 238, 287],\n",
              " [211, 4, 482, 75, 2, 238, 287, 920],\n",
              " [211, 4, 482, 75, 2, 238, 287, 920, 122],\n",
              " [211, 4, 482, 75, 2, 238, 287, 920, 122, 239],\n",
              " [12, 3],\n",
              " [12, 3, 1],\n",
              " [12, 3, 1, 164],\n",
              " [12, 3, 1, 164, 78],\n",
              " [43, 5],\n",
              " [43, 5, 1],\n",
              " [43, 5, 1, 99],\n",
              " [43, 5, 1, 99, 212],\n",
              " [43, 5, 1, 99, 212, 18],\n",
              " [43, 5, 1, 99, 212, 18, 9],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3, 2],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3, 2, 164],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3, 2, 164, 78],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3, 2, 164, 78, 921],\n",
              " [43, 5, 1, 99, 212, 18, 9, 101, 3, 2, 164, 78, 921, 2],\n",
              " [922, 640],\n",
              " [922, 640, 5],\n",
              " [922, 640, 5, 1],\n",
              " [922, 640, 5, 1, 604],\n",
              " [922, 640, 5, 1, 604, 160],\n",
              " [922, 640, 5, 1, 604, 160, 21],\n",
              " [922, 640, 5, 1, 604, 160, 21, 27],\n",
              " [922, 640, 5, 1, 604, 160, 21, 27, 179],\n",
              " [922, 640, 5, 1, 604, 160, 21, 27, 179, 2],\n",
              " [922, 640, 5, 1, 604, 160, 21, 27, 179, 2, 641],\n",
              " [922, 640, 5, 1, 604, 160, 21, 27, 179, 2, 641, 34],\n",
              " [19, 6],\n",
              " [19, 6, 2],\n",
              " [19, 6, 2, 61],\n",
              " [19, 6, 2, 61, 19],\n",
              " [19, 6, 2, 61, 19, 109],\n",
              " [19, 6, 2, 61, 19, 109, 2],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16, 642],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16, 642, 24],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16, 642, 24, 183],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16, 642, 24, 183, 131],\n",
              " [19, 6, 2, 61, 19, 109, 2, 16, 642, 24, 183, 131, 643],\n",
              " [20, 4],\n",
              " [20, 4, 288],\n",
              " [20, 4, 288, 2],\n",
              " [20, 4, 288, 2, 184],\n",
              " [20, 4, 288, 2, 184, 145],\n",
              " [20, 4, 288, 2, 184, 145, 31],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182, 10],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182, 10, 397],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182, 10, 397, 398],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182, 10, 397, 398, 47],\n",
              " [20, 4, 288, 2, 184, 145, 31, 40, 29, 644, 182, 10, 397, 398, 47, 123],\n",
              " [132, 79],\n",
              " [132, 79, 182],\n",
              " [132, 79, 182, 5],\n",
              " [132, 79, 182, 5, 1],\n",
              " [132, 79, 182, 5, 1, 54],\n",
              " [132, 79, 182, 5, 1, 54, 19],\n",
              " [132, 79, 182, 5, 1, 54, 19, 10],\n",
              " [132, 79, 182, 5, 1, 54, 19, 10, 397],\n",
              " [12, 3],\n",
              " [12, 3, 1],\n",
              " [12, 3, 1, 646],\n",
              " [12, 3, 1, 646, 5],\n",
              " [12, 3, 1, 646, 5, 36],\n",
              " [12, 3, 1, 646, 5, 36, 80],\n",
              " [12, 3, 1, 646, 5, 36, 80, 7],\n",
              " [12, 3, 1, 646, 5, 36, 80, 7, 2],\n",
              " [12, 3, 1, 646, 5, 36, 80, 7, 2, 16],\n",
              " [12, 3, 1, 646, 5, 36, 80, 7, 2, 16, 11],\n",
              " [46, 1],\n",
              " [46, 1, 99],\n",
              " [46, 1, 99, 212],\n",
              " [46, 1, 99, 212, 333],\n",
              " [46, 1, 99, 212, 333, 26],\n",
              " [46, 1, 99, 212, 333, 26, 36],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483, 288],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483, 288, 2],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483, 288, 2, 184],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483, 288, 2, 184, 145],\n",
              " [46, 1, 99, 212, 333, 26, 36, 14, 483, 288, 2, 184, 145, 31],\n",
              " [923, 29],\n",
              " [923, 29, 52],\n",
              " [923, 29, 52, 8],\n",
              " [923, 29, 52, 8, 924],\n",
              " [923, 29, 52, 8, 924, 1],\n",
              " [923, 29, 52, 8, 924, 1, 647],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6, 91],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6, 91, 20],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6, 91, 20, 34],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6, 91, 20, 34, 4],\n",
              " [923, 29, 52, 8, 924, 1, 647, 334, 5, 1, 116, 6, 91, 20, 34, 4, 165],\n",
              " [36, 14],\n",
              " [36, 14, 90],\n",
              " [36, 14, 90, 14],\n",
              " [36, 14, 90, 14, 185],\n",
              " [36, 14, 90, 14, 185, 63],\n",
              " [36, 14, 90, 14, 185, 63, 335],\n",
              " [36, 14, 90, 14, 185, 63, 335, 6],\n",
              " [36, 14, 90, 14, 185, 63, 335, 6, 133],\n",
              " [36, 14, 90, 14, 185, 63, 335, 6, 133, 10],\n",
              " [36, 14, 90, 14, 185, 63, 335, 6, 133, 10, 289],\n",
              " [5, 36],\n",
              " [5, 36, 80],\n",
              " [12, 3],\n",
              " [12, 3, 1],\n",
              " [12, 3, 1, 88],\n",
              " [12, 3, 1, 88, 14],\n",
              " [96, 399],\n",
              " [96, 399, 4],\n",
              " [96, 399, 4, 20],\n",
              " [96, 399, 4, 20, 963],\n",
              " [96, 399, 4, 20, 963, 29],\n",
              " [96, 399, 4, 20, 963, 29, 87],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2, 927],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2, 927, 4],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2, 927, 4, 400],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2, 927, 4, 400, 45],\n",
              " [96, 399, 4, 20, 963, 29, 87, 88, 14, 3, 2, 927, 4, 400, 45, 242],\n",
              " [125, 101],\n",
              " [125, 101, 213],\n",
              " [125, 101, 213, 3],\n",
              " [125, 101, 213, 3, 97],\n",
              " [125, 101, 213, 3, 97, 32],\n",
              " [125, 101, 213, 3, 97, 32, 4],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1, 87],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1, 87, 5],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1, 87, 5, 1],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1, 87, 5, 1, 35],\n",
              " [125, 101, 213, 3, 97, 32, 4, 401, 1, 87, 5, 1, 35, 19],\n",
              " [243, 86],\n",
              " [243, 86, 15],\n",
              " [243, 86, 15, 928],\n",
              " [243, 86, 15, 928, 24],\n",
              " [243, 86, 15, 928, 24, 87],\n",
              " [243, 86, 15, 928, 24, 87, 285],\n",
              " [243, 86, 15, 928, 24, 87, 285, 290],\n",
              " [243, 86, 15, 928, 24, 87, 285, 290, 1],\n",
              " [243, 86, 15, 928, 24, 87, 285, 290, 1, 16],\n",
              " [243, 86, 15, 928, 24, 87, 285, 290, 1, 16, 11],\n",
              " [6, 64],\n",
              " [6, 64, 24],\n",
              " [6, 64, 24, 243],\n",
              " [6, 64, 24, 243, 1],\n",
              " [6, 64, 24, 243, 1, 115],\n",
              " [6, 64, 24, 243, 1, 115, 49],\n",
              " [6, 64, 24, 243, 1, 115, 49, 80],\n",
              " [12, 3],\n",
              " [12, 3, 30],\n",
              " [12, 3, 30, 58],\n",
              " [30, 58],\n",
              " [30, 58, 3],\n",
              " [30, 58, 3, 26],\n",
              " [30, 58, 3, 26, 649],\n",
              " [30, 58, 3, 26, 649, 67],\n",
              " [30, 58, 3, 26, 649, 67, 4],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291, 1],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291, 1, 88],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291, 1, 88, 14],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291, 1, 88, 14, 29],\n",
              " [30, 58, 3, 26, 649, 67, 4, 291, 1, 88, 14, 29, 4],\n",
              " [291, 26],\n",
              " [291, 26, 87],\n",
              " [291, 26, 87, 1],\n",
              " [291, 26, 87, 1, 484],\n",
              " [291, 26, 87, 1, 484, 3],\n",
              " [291, 26, 87, 1, 484, 3, 4],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1, 486],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1, 486, 5],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1, 486, 5, 2],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1, 486, 5, 2, 14],\n",
              " [291, 26, 87, 1, 484, 3, 4, 485, 1, 486, 5, 2, 14, 21],\n",
              " [487, 1],\n",
              " [487, 1, 488],\n",
              " [487, 1, 488, 1],\n",
              " [487, 1, 488, 1, 27],\n",
              " [487, 1, 488, 1, 27, 145],\n",
              " [487, 1, 488, 1, 27, 145, 292],\n",
              " [487, 1, 488, 1, 27, 145, 292, 4],\n",
              " [487, 1, 488, 1, 27, 145, 292, 4, 402],\n",
              " [487, 1, 488, 1, 27, 145, 292, 4, 402, 1],\n",
              " [487, 1, 488, 1, 27, 145, 292, 4, 402, 1, 87],\n",
              " [12, 89],\n",
              " [12, 89, 23],\n",
              " [12, 89, 23, 146],\n",
              " [12, 89, 23, 146, 37],\n",
              " [12, 89, 23, 146, 37, 86],\n",
              " [86, 3],\n",
              " [86, 3, 2],\n",
              " [86, 3, 2, 244],\n",
              " [86, 3, 2, 244, 4],\n",
              " [86, 3, 2, 244, 4, 245],\n",
              " [86, 3, 2, 244, 4, 245, 1],\n",
              " [86, 3, 2, 244, 4, 245, 1, 213],\n",
              " [86, 3, 2, 244, 4, 245, 1, 213, 5],\n",
              " [86, 3, 2, 244, 4, 245, 1, 213, 5, 1],\n",
              " [86, 3, 2, 244, 4, 245, 1, 213, 5, 1, 11],\n",
              " [86, 3, 2, 244, 4, 245, 1, 213, 5, 1, 11, 8],\n",
              " [932, 1],\n",
              " [932, 1, 87],\n",
              " [932, 1, 87, 6],\n",
              " [932, 1, 87, 6, 147],\n",
              " [932, 1, 87, 6, 147, 1],\n",
              " [932, 1, 87, 6, 147, 1, 38],\n",
              " [932, 1, 87, 6, 147, 1, 38, 4],\n",
              " [932, 1, 87, 6, 147, 1, 38, 4, 402],\n",
              " [932, 1, 87, 6, 147, 1, 38, 4, 402, 1],\n",
              " [932, 1, 87, 6, 147, 1, 38, 4, 402, 1, 87],\n",
              " [12, 3],\n",
              " [12, 3, 1],\n",
              " [12, 3, 1, 186],\n",
              " [12, 3, 1, 186, 81],\n",
              " [12, 3, 1, 186, 81, 2],\n",
              " [12, 3, 1, 186, 81, 2, 403],\n",
              " [12, 3, 1, 186, 81, 2, 403, 16],\n",
              " [12, 3, 1, 186, 81, 2, 403, 16, 11],\n",
              " [12, 3, 1, 186, 81, 2, 403, 16, 11, 6],\n",
              " [110, 16],\n",
              " [110, 16, 11],\n",
              " [2, 403],\n",
              " [2, 403, 16],\n",
              " [2, 403, 16, 11],\n",
              " [2, 403, 16, 11, 651],\n",
              " [2, 403, 16, 11, 651, 933],\n",
              " [2, 403, 16, 11, 651, 933, 7],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43, 488],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43, 488, 62],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43, 488, 62, 34],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43, 488, 62, 34, 4],\n",
              " [2, 403, 16, 11, 651, 933, 7, 43, 488, 62, 34, 4, 35],\n",
              " [65, 10],\n",
              " [65, 10, 132],\n",
              " [65, 10, 132, 490],\n",
              " [65, 10, 132, 490, 934],\n",
              " [65, 10, 132, 490, 934, 1],\n",
              " [65, 10, 132, 490, 934, 1, 11],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98, 1],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98, 1, 652],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98, 1, 652, 34],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98, 1, 652, 34, 8],\n",
              " [65, 10, 132, 490, 934, 1, 11, 935, 98, 1, 652, 34, 8, 336],\n",
              " [936, 337],\n",
              " [936, 337, 116],\n",
              " [936, 337, 116, 95],\n",
              " [12, 10],\n",
              " [12, 10, 1],\n",
              " [12, 10, 1, 339],\n",
              " [12, 10, 1, 339, 5],\n",
              " [12, 10, 1, 339, 5, 2],\n",
              " [12, 10, 1, 339, 5, 2, 110],\n",
              " [12, 10, 1, 339, 5, 2, 110, 16],\n",
              " [12, 10, 1, 339, 5, 2, 110, 16, 11],\n",
              " [12, 10, 1, 339, 5, 2, 110, 16, 11, 85],\n",
              " [1, 85],\n",
              " [1, 85, 22],\n",
              " [1, 85, 22, 31],\n",
              " [1, 85, 22, 31, 32],\n",
              " [1, 85, 22, 31, 32, 17],\n",
              " [1, 85, 22, 31, 32, 17, 404],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294, 246],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294, 246, 938],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294, 246, 938, 6],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294, 246, 938, 6, 53],\n",
              " [1, 85, 22, 31, 32, 17, 404, 294, 246, 938, 6, 53, 939],\n",
              " [110, 16],\n",
              " [110, 16, 41],\n",
              " [110, 16, 41, 22],\n",
              " [110, 16, 41, 22, 96],\n",
              " [110, 16, 41, 22, 96, 940],\n",
              " [110, 16, 41, 22, 96, 940, 92],\n",
              " [110, 16, 41, 22, 96, 940, 92, 491],\n",
              " [110, 16, 41, 22, 96, 940, 92, 491, 295],\n",
              " [110, 16, 41, 22, 96, 940, 92, 491, 295, 117],\n",
              " [110, 16, 41, 22, 96, 940, 92, 491, 295, 117, 20],\n",
              " [653, 1],\n",
              " [653, 1, 941],\n",
              " [653, 1, 941, 5],\n",
              " [653, 1, 941, 5, 942],\n",
              " [653, 1, 941, 5, 942, 7],\n",
              " [653, 1, 941, 5, 942, 7, 2],\n",
              " [653, 1, 941, 5, 942, 7, 2, 943],\n",
              " [653, 1, 941, 5, 942, 7, 2, 943, 29],\n",
              " [653, 1, 941, 5, 942, 7, 2, 943, 29, 944],\n",
              " [12, 10],\n",
              " [12, 10, 1],\n",
              " [12, 10, 1, 133],\n",
              " [12, 10, 1, 133, 6],\n",
              " [12, 10, 1, 133, 6, 63],\n",
              " [12, 10, 1, 133, 6, 63, 80],\n",
              " [133, 3],\n",
              " [133, 3, 26],\n",
              " [133, 3, 26, 36],\n",
              " [133, 3, 26, 36, 14],\n",
              " [133, 3, 26, 36, 14, 24],\n",
              " [133, 3, 26, 36, 14, 24, 946],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35, 81],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35, 81, 148],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35, 81, 148, 6],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35, 81, 148, 6, 43],\n",
              " [133, 3, 26, 36, 14, 24, 946, 1, 35, 81, 148, 6, 43, 8],\n",
              " [947, 48],\n",
              " [947, 48, 35],\n",
              " [947, 48, 35, 117],\n",
              " [947, 48, 35, 117, 24],\n",
              " [947, 48, 35, 117, 24, 1],\n",
              " [947, 48, 35, 117, 24, 1, 948],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296, 3],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296, 3, 405],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296, 3, 405, 4],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296, 3, 405, 4, 43],\n",
              " [947, 48, 35, 117, 24, 1, 948, 334, 5, 1, 296, 3, 405, 4, 43, 133],\n",
              " [3, 286],\n",
              " [3, 286, 32],\n",
              " [3, 286, 32, 17],\n",
              " [3, 286, 32, 17, 35],\n",
              " [3, 286, 32, 17, 35, 47],\n",
              " [63, 29],\n",
              " [63, 29, 406],\n",
              " [63, 29, 406, 100],\n",
              " [63, 29, 406, 100, 247],\n",
              " [63, 29, 406, 100, 247, 3],\n",
              " [63, 29, 406, 100, 247, 3, 1],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32, 36],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32, 36, 14],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32, 36, 14, 8],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32, 36, 14, 8, 407],\n",
              " [63, 29, 406, 100, 247, 3, 1, 99, 654, 32, 36, 14, 8, 407, 26],\n",
              " [35, 5],\n",
              " [35, 5, 76],\n",
              " [35, 5, 76, 51],\n",
              " [35, 5, 76, 51, 76],\n",
              " [35, 5, 76, 51, 76, 3],\n",
              " [35, 5, 76, 51, 76, 3, 949],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3, 286],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3, 286, 32],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3, 286, 32, 17],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3, 286, 32, 17, 61],\n",
              " [35, 5, 76, 51, 76, 3, 949, 6, 950, 492, 63, 3, 286, 32, 17, 61, 47],\n",
              " [12, 10],\n",
              " [12, 10, 493],\n",
              " [28, 16],\n",
              " [28, 16, 41],\n",
              " [28, 16, 41, 297],\n",
              " [28, 16, 41, 297, 655],\n",
              " [28, 16, 41, 297, 655, 28],\n",
              " [28, 16, 41, 297, 655, 28, 493],\n",
              " [28, 16, 41, 297, 655, 28, 493, 494],\n",
              " [28, 16, 41, 297, 655, 28, 493, 494, 1],\n",
              " [28, 16, 41, 297, 655, 28, 493, 494, 1, 13],\n",
              " [28, 16, 41, 297, 655, 28, 493, 494, 1, 13, 3],\n",
              " [953, 954],\n",
              " [953, 954, 2],\n",
              " [953, 954, 2, 656],\n",
              " [953, 954, 2, 656, 3],\n",
              " [953, 954, 2, 656, 3, 2],\n",
              " [953, 954, 2, 656, 3, 2, 340],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657, 68],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657, 68, 3],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657, 68, 3, 126],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657, 68, 3, 126, 341],\n",
              " [953, 954, 2, 656, 3, 2, 340, 657, 68, 3, 126, 341, 1],\n",
              " [9, 124],\n",
              " [9, 124, 955],\n",
              " [9, 124, 955, 8],\n",
              " [9, 124, 955, 8, 487],\n",
              " [9, 124, 955, 8, 487, 45],\n",
              " [9, 124, 955, 8, 487, 45, 2],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3, 495],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3, 495, 6],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3, 495, 6, 1],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3, 495, 6, 1, 284],\n",
              " [9, 124, 955, 8, 487, 45, 2, 11, 3, 495, 6, 1, 284, 5],\n",
              " [1, 11],\n",
              " [1, 11, 117],\n",
              " [1, 11, 117, 20],\n",
              " [1, 11, 117, 20, 1],\n",
              " [1, 11, 117, 20, 1, 187],\n",
              " [1, 11, 117, 20, 1, 187, 5],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496, 1],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496, 1, 9],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496, 1, 9, 248],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496, 1, 9, 248, 956],\n",
              " [1, 11, 117, 20, 1, 187, 5, 61, 496, 1, 9, 248, 956, 234],\n",
              " [12, 59],\n",
              " [12, 59, 658],\n",
              " [12, 59, 658, 51],\n",
              " [12, 59, 658, 51, 1],\n",
              " [12, 59, 658, 51, 1, 9],\n",
              " [12, 59, 658, 51, 1, 9, 248],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126, 127],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126, 127, 408],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126, 127, 408, 29],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126, 127, 408, 29, 127],\n",
              " [12, 59, 658, 51, 1, 9, 248, 3, 126, 127, 408, 29, 127, 188],\n",
              " [57, 125],\n",
              " [57, 125, 9],\n",
              " [57, 125, 9, 248],\n",
              " [57, 125, 9, 248, 3],\n",
              " [57, 125, 9, 248, 3, 127],\n",
              " [57, 125, 9, 248, 3, 127, 408],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27, 59],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27, 59, 958],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27, 59, 958, 249],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27, 59, 958, 249, 497],\n",
              " [57, 125, 9, 248, 3, 127, 408, 49, 5, 1, 27, 59, 958, 249, 497, 20],\n",
              " [15, 10],\n",
              " [15, 10, 342],\n",
              " [15, 10, 342, 659],\n",
              " [15, 10, 342, 659, 147],\n",
              " [15, 10, 342, 659, 147, 4],\n",
              " [15, 10, 342, 659, 147, 4, 1],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8, 59],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8, 59, 292],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8, 59, 292, 166],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8, 59, 292, 166, 147],\n",
              " [15, 10, 342, 659, 147, 4, 1, 38, 8, 59, 292, 166, 147, 341],\n",
              " [959, 1],\n",
              " [959, 1, 498],\n",
              " [959, 1, 498, 189],\n",
              " [51, 1],\n",
              " [51, 1, 9],\n",
              " [51, 1, 9, 248],\n",
              " [51, 1, 9, 248, 3],\n",
              " [51, 1, 9, 248, 3, 126],\n",
              " [51, 1, 9, 248, 3, 126, 127],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960, 961],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960, 961, 962],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960, 961, 962, 499],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960, 961, 962, 499, 4],\n",
              " [51, 1, 9, 248, 3, 126, 127, 188, 21, 960, 961, 962, 499, 4, 1],\n",
              " [963, 14],\n",
              " [963, 14, 660],\n",
              " [963, 14, 660, 4],\n",
              " [963, 14, 660, 4, 964],\n",
              " [963, 14, 660, 4, 964, 147],\n",
              " [963, 14, 660, 4, 964, 147, 7],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409, 965],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409, 965, 4],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409, 965, 4, 343],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409, 965, 4, 343, 27],\n",
              " [963, 14, 660, 4, 964, 147, 7, 38, 8, 409, 965, 4, 343, 27, 22],\n",
              " [149, 2],\n",
              " [149, 2, 242],\n",
              " [149, 2, 242, 35],\n",
              " [149, 2, 242, 35, 29],\n",
              " [149, 2, 242, 35, 29, 250],\n",
              " [149, 2, 242, 35, 29, 250, 966],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967, 17],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967, 17, 1],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967, 17, 1, 11],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967, 17, 1, 11, 4],\n",
              " [149, 2, 242, 35, 29, 250, 966, 13, 3, 127, 967, 17, 1, 11, 4, 158],\n",
              " [12, 3],\n",
              " [12, 3, 214],\n",
              " [12, 3, 214, 6],\n",
              " [12, 3, 214, 6, 56],\n",
              " [12, 3, 214, 6, 56, 162],\n",
              " [214, 3],\n",
              " [214, 3, 2],\n",
              " [214, 3, 2, 244],\n",
              " [214, 3, 2, 244, 5],\n",
              " [214, 3, 2, 244, 5, 661],\n",
              " [214, 3, 2, 244, 5, 661, 251],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6, 641],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6, 641, 496],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6, 641, 496, 5],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6, 641, 496, 5, 2],\n",
              " [214, 3, 2, 244, 5, 661, 251, 61, 6, 641, 496, 5, 2, 11],\n",
              " [252, 4],\n",
              " [252, 4, 969],\n",
              " [252, 4, 969, 253],\n",
              " [252, 4, 969, 253, 5],\n",
              " [252, 4, 969, 253, 5, 13],\n",
              " [252, 4, 969, 253, 5, 13, 662],\n",
              " [252, 4, 969, 253, 5, 13, 662, 661],\n",
              " [252, 4, 969, 253, 5, 13, 662, 661, 970],\n",
              " [252, 4, 969, 253, 5, 13, 662, 661, 970, 5],\n",
              " [252, 4, 969, 253, 5, 13, 662, 661, 970, 5, 1],\n",
              " [252, 4, 969, 253, 5, 13, 662, 661, 970, 5, 1, 182],\n",
              " [8, 971],\n",
              " [8, 971, 1],\n",
              " [8, 971, 1, 187],\n",
              " [8, 971, 1, 187, 5],\n",
              " [8, 971, 1, 187, 5, 500],\n",
              " [8, 971, 1, 187, 5, 500, 664],\n",
              " [8, 971, 1, 187, 5, 500, 664, 4],\n",
              " [8, 971, 1, 187, 5, 500, 664, 4, 343],\n",
              " [8, 971, 1, 187, 5, 500, 664, 4, 343, 1],\n",
              " [8, 971, 1, 187, 5, 500, 664, 4, 343, 1, 11],\n",
              " [56, 162],\n",
              " [56, 162, 3],\n",
              " [56, 162, 3, 1],\n",
              " [56, 162, 3, 1, 244],\n",
              " [56, 162, 3, 1, 244, 4],\n",
              " [56, 162, 3, 1, 244, 4, 245],\n",
              " [56, 162, 3, 1, 244, 4, 245, 1],\n",
              " [56, 162, 3, 1, 244, 4, 245, 1, 213],\n",
              " [56, 162, 3, 1, 244, 4, 245, 1, 213, 6],\n",
              " [56, 162, 3, 1, 244, 4, 245, 1, 213, 6, 972],\n",
              " [56, 162, 3, 1, 244, 4, 245, 1, 213, 6, 972, 5],\n",
              " [16, 41],\n",
              " [16, 41, 37],\n",
              " [16, 41, 37, 665],\n",
              " [16, 41, 37, 665, 1],\n",
              " [16, 41, 37, 665, 1, 116],\n",
              " [16, 41, 37, 665, 1, 116, 7],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19, 111],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19, 111, 24],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19, 111, 24, 118],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19, 111, 24, 118, 42],\n",
              " [16, 41, 37, 665, 1, 116, 7, 190, 19, 111, 24, 118, 42, 191],\n",
              " [35, 36],\n",
              " [35, 36, 5],\n",
              " [35, 36, 5, 148],\n",
              " [35, 36, 5, 148, 6],\n",
              " [35, 36, 5, 148, 6, 501],\n",
              " [35, 36, 5, 148, 6, 501, 666],\n",
              " [35, 36, 5, 148, 6, 501, 666, 5],\n",
              " [35, 36, 5, 148, 6, 501, 666, 5, 43],\n",
              " [12, 3],\n",
              " [12, 3, 1],\n",
              " [12, 3, 1, 186],\n",
              " [12, 3, 1, 186, 81],\n",
              " [12, 3, 1, 186, 81, 56],\n",
              " [12, 3, 1, 186, 81, 56, 30],\n",
              " [12, 3, 1, 186, 81, 56, 30, 58],\n",
              " [12, 3, 1, 186, 81, 56, 30, 58, 6],\n",
              " [12, 3, 1, 186, 81, 56, 30, 58, 6, 131],\n",
              " [30, 58],\n",
              " [56, 30],\n",
              " [56, 30, 58],\n",
              " [131, 30],\n",
              " [131, 30, 58],\n",
              " [1, 56],\n",
              " [1, 56, 30],\n",
              " [1, 56, 30, 668],\n",
              " [1, 56, 30, 668, 1],\n",
              " [1, 56, 30, 668, 1, 30],\n",
              " [1, 56, 30, 668, 1, 30, 66],\n",
              " [1, 56, 30, 668, 1, 30, 66, 1],\n",
              " [150, 167],\n",
              " [8, 329],\n",
              " [8, 329, 92],\n",
              " [8, 329, 92, 4],\n",
              " [8, 329, 92, 4, 343],\n",
              " [8, 329, 92, 4, 343, 168],\n",
              " [8, 329, 92, 4, 343, 168, 1],\n",
              " [8, 329, 92, 4, 343, 168, 1, 669],\n",
              " [8, 329, 92, 4, 343, 168, 1, 669, 5],\n",
              " [8, 329, 92, 4, 343, 168, 1, 669, 5, 13],\n",
              " [8, 329, 92, 4, 343, 168, 1, 669, 5, 13, 3],\n",
              " [670, 6],\n",
              " [670, 6, 38],\n",
              " [670, 6, 38, 215],\n",
              " [670, 6, 38, 215, 497],\n",
              " [1, 131],\n",
              " [1, 131, 30],\n",
              " [668, 1],\n",
              " [668, 1, 30],\n",
              " [668, 1, 30, 66],\n",
              " [668, 1, 30, 66, 2],\n",
              " [161, 671],\n",
              " [8, 973],\n",
              " [8, 973, 143],\n",
              " [8, 973, 143, 282],\n",
              " [8, 973, 143, 282, 119],\n",
              " [1, 56],\n",
              " [1, 56, 30],\n",
              " [1, 56, 30, 168],\n",
              " [1, 56, 30, 168, 8],\n",
              " [147, 128],\n",
              " [147, 128, 74],\n",
              " [147, 128, 74, 974],\n",
              " [12, 3],\n",
              " [12, 3, 253],\n",
              " [12, 3, 253, 6],\n",
              " [12, 3, 253, 6, 254],\n",
              " [12, 3, 253, 6, 254, 6],\n",
              " [12, 3, 253, 6, 254, 6, 45],\n",
              " [12, 3, 253, 6, 254, 6, 45, 4],\n",
              " [12, 3, 253, 6, 254, 6, 45, 4, 672],\n",
              " [12, 3, 253, 6, 254, 6, 45, 4, 672, 151],\n",
              " [253, 502],\n",
              " [253, 502, 57],\n",
              " [253, 502, 57, 1],\n",
              " [253, 502, 57, 1, 27],\n",
              " [253, 502, 57, 1, 27, 344],\n",
              " [253, 502, 57, 1, 27, 344, 1],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345, 7],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345, 7, 1],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345, 7, 1, 49],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345, 7, 1, 49, 13],\n",
              " [253, 502, 57, 1, 27, 344, 1, 410, 6, 345, 7, 1, 49, 13, 4],\n",
              " [1, 976],\n",
              " [1, 976, 24],\n",
              " [1, 976, 24, 8],\n",
              " [1, 976, 24, 8, 977],\n",
              " [1, 976, 24, 8, 977, 978],\n",
              " [1, 976, 24, 8, 977, 978, 1],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5, 1],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5, 1, 27],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5, 1, 27, 40],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5, 1, 27, 40, 255],\n",
              " [1, 976, 24, 8, 977, 978, 1, 979, 5, 1, 27, 40, 255, 107],\n",
              " [8, 3],\n",
              " [8, 3, 74],\n",
              " [8, 3, 74, 503],\n",
              " [8, 3, 74, 503, 4],\n",
              " [8, 3, 74, 503, 4, 673],\n",
              " [8, 3, 74, 503, 4, 673, 28],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101, 24],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101, 24, 42],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101, 24, 42, 74],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101, 24, 42, 74, 980],\n",
              " [8, 3, 74, 503, 4, 673, 28, 393, 101, 24, 42, 74, 980, 57],\n",
              " [9, 2],\n",
              " [9, 2, 346],\n",
              " [9, 2, 346, 14],\n",
              " [9, 2, 346, 14, 26],\n",
              " [9, 2, 346, 14, 26, 120],\n",
              " [9, 2, 346, 14, 26, 120, 69],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27, 3],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27, 3, 674],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27, 3, 674, 46],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27, 3, 674, 46, 981],\n",
              " [9, 2, 346, 14, 26, 120, 69, 31, 51, 2, 27, 3, 674, 46, 981, 6],\n",
              " [675, 123],\n",
              " [675, 123, 98],\n",
              " [675, 123, 98, 676],\n",
              " [675, 123, 98, 676, 675],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "input_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDINihM2MskQ"
      },
      "source": [
        "# prepadding with zeros to the input sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDC1zRHhJfGs"
      },
      "outputs": [],
      "source": [
        "max_length=max([len(x) for x in input_questions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctlAu0txM7uV",
        "outputId": "5d64fdd6-8536-414f-e997-4eb4ed20bc4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,   12,    3],\n",
              "       [   0,    0,    0, ...,   12,    3,   18],\n",
              "       [   0,    0,    0, ...,    3,   18,    9],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    3, 1774,   37],\n",
              "       [   0,    0,    0, ..., 1774,   37,    2],\n",
              "       [   0,    0,    0, ...,   37,    2,  746]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "padded_input_questions=pre_pad=pad_sequences(input_questions,maxlen=max_length,padding='pre')\n",
        "padded_input_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1QziTwRNY1U"
      },
      "source": [
        "# Splitting X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWKKMHU9NV0k",
        "outputId": "43de763c-e005-41c0-960f-ba4995cc6312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8853, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "X=padded_input_questions[:,:-1]\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnywRU-hNkRw",
        "outputId": "20bde962-b896-4f19-9646-b6eac03f9e55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8853,)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "y=padded_input_questions[:,-1]\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95S8QoUoN436"
      },
      "source": [
        "# we converted our data to supervised learning ,classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68COc9k2Nr8l"
      },
      "outputs": [],
      "source": [
        "# perfroming ohe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djsrg6-kPbCU"
      },
      "outputs": [],
      "source": [
        "y_ohe=to_categorical(y,num_classes=total_words + 1) #we have index 1 so when adding + 1 will give all the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqlJsvcQFTP",
        "outputId": "443e1d92-e9ef-4457-c76c-db47efc96e8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8853,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nSs__XKQuVT"
      },
      "source": [
        "# Building the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkaMJwJJTOwY",
        "outputId": "023703d2-b802-4a14-83d8-ca8b52be89d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "277/277 [==============================] - 17s 42ms/step - loss: 6.2373 - accuracy: 0.0749\n",
            "Epoch 2/10\n",
            "277/277 [==============================] - 5s 17ms/step - loss: 5.8721 - accuracy: 0.0757\n",
            "Epoch 3/10\n",
            "277/277 [==============================] - 3s 12ms/step - loss: 5.7509 - accuracy: 0.0757\n",
            "Epoch 4/10\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 5.6292 - accuracy: 0.0813\n",
            "Epoch 5/10\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 5.5146 - accuracy: 0.0863\n",
            "Epoch 6/10\n",
            "277/277 [==============================] - 4s 14ms/step - loss: 5.4169 - accuracy: 0.0915\n",
            "Epoch 7/10\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 5.3160 - accuracy: 0.1029\n",
            "Epoch 8/10\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 5.2077 - accuracy: 0.1168\n",
            "Epoch 9/10\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 5.1036 - accuracy: 0.1278\n",
            "Epoch 10/10\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 5.0086 - accuracy: 0.1376\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a35af5fe5c0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words + 1, 100, input_length=X.shape[1]))\n",
        "#return_sequences -- True when we want to add another LSTM Layer after this\n",
        "#return_sequences -- False -- When we are adding a Dense layer after LSTM\n",
        "model.add(LSTM(units=150, return_sequences=True, kernel_initializer=\"random_uniform\"))\n",
        "model.add(LSTM(units=80, return_sequences=False, kernel_initializer=\"random_uniform\"))\n",
        "model.add(Dense(units=total_words+1, activation=\"softmax\", kernel_initializer=\"random_uniform\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Assuming X and y are your input and target data\n",
        "y_one_hot = to_categorical(y, num_classes= total_words + 1)\n",
        "model.fit(X, y_one_hot, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6L71iqW6vX"
      },
      "source": [
        "# got very low accuracy\n",
        "# Need to retrain the model and adding dropout,earlystopping and making it to 200 epochs,will give best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUGPMjkcR_vO",
        "outputId": "929cd60e-e4ff-4a5d-fdff-1162f53bbcd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 27, 100)           177500    \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 27, 150)           150600    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 80)                73920     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1775)              143775    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 545795 (2.08 MB)\n",
            "Trainable params: 545795 (2.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "RS5f0j6PSBcd",
        "outputId": "2217c8c6-5065-4c31-f475-414e202aa6d3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plot_model(model,show_shapes = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqi8rcj_XdKA",
        "outputId": "359044ba-1fc2-46cd-9afe-e3e6ed095315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "277/277 [==============================] - 17s 39ms/step - loss: 6.1890 - accuracy: 0.0735\n",
            "Epoch 2/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 5.8311 - accuracy: 0.0757\n",
            "Epoch 3/200\n",
            "277/277 [==============================] - 4s 13ms/step - loss: 5.7336 - accuracy: 0.0766\n",
            "Epoch 4/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 5.6035 - accuracy: 0.0877\n",
            "Epoch 5/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 5.4438 - accuracy: 0.1066\n",
            "Epoch 6/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 5.3068 - accuracy: 0.1180\n",
            "Epoch 7/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 5.1817 - accuracy: 0.1299\n",
            "Epoch 8/200\n",
            "277/277 [==============================] - 3s 12ms/step - loss: 5.0630 - accuracy: 0.1393\n",
            "Epoch 9/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 4.9535 - accuracy: 0.1433\n",
            "Epoch 10/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 4.8624 - accuracy: 0.1485\n",
            "Epoch 11/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 4.7642 - accuracy: 0.1580\n",
            "Epoch 12/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 4.6772 - accuracy: 0.1606\n",
            "Epoch 13/200\n",
            "277/277 [==============================] - 3s 12ms/step - loss: 4.5825 - accuracy: 0.1683\n",
            "Epoch 14/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 4.5020 - accuracy: 0.1728\n",
            "Epoch 15/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 4.4177 - accuracy: 0.1754\n",
            "Epoch 16/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 4.3387 - accuracy: 0.1815\n",
            "Epoch 17/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 4.2515 - accuracy: 0.1889\n",
            "Epoch 18/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 4.1769 - accuracy: 0.1953\n",
            "Epoch 19/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 4.0993 - accuracy: 0.1990\n",
            "Epoch 20/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 4.0197 - accuracy: 0.2022\n",
            "Epoch 21/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.9488 - accuracy: 0.2068\n",
            "Epoch 22/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.8802 - accuracy: 0.2084\n",
            "Epoch 23/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 3.8053 - accuracy: 0.2159\n",
            "Epoch 24/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 4.1283 - accuracy: 0.1965\n",
            "Epoch 25/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 4.6747 - accuracy: 0.1607\n",
            "Epoch 26/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 4.1257 - accuracy: 0.1942\n",
            "Epoch 27/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.9222 - accuracy: 0.2019\n",
            "Epoch 28/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.7855 - accuracy: 0.2161\n",
            "Epoch 29/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 3.6792 - accuracy: 0.2227\n",
            "Epoch 30/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 3.5738 - accuracy: 0.2328\n",
            "Epoch 31/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 3.4912 - accuracy: 0.2435\n",
            "Epoch 32/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.4028 - accuracy: 0.2596\n",
            "Epoch 33/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.3095 - accuracy: 0.2662\n",
            "Epoch 34/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 3.2225 - accuracy: 0.2809\n",
            "Epoch 35/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 3.1431 - accuracy: 0.2879\n",
            "Epoch 36/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 3.0669 - accuracy: 0.3034\n",
            "Epoch 37/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 2.9953 - accuracy: 0.3182\n",
            "Epoch 38/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.9097 - accuracy: 0.3267\n",
            "Epoch 39/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 2.8394 - accuracy: 0.3453\n",
            "Epoch 40/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 2.7704 - accuracy: 0.3519\n",
            "Epoch 41/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.6956 - accuracy: 0.3685\n",
            "Epoch 42/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.6262 - accuracy: 0.3876\n",
            "Epoch 43/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.5465 - accuracy: 0.3983\n",
            "Epoch 44/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.4920 - accuracy: 0.4077\n",
            "Epoch 45/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 2.4333 - accuracy: 0.4244\n",
            "Epoch 46/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.3557 - accuracy: 0.4420\n",
            "Epoch 47/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.3087 - accuracy: 0.4436\n",
            "Epoch 48/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.2314 - accuracy: 0.4642\n",
            "Epoch 49/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 2.1616 - accuracy: 0.4819\n",
            "Epoch 50/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 2.1113 - accuracy: 0.4916\n",
            "Epoch 51/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 2.0631 - accuracy: 0.4970\n",
            "Epoch 52/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.9970 - accuracy: 0.5194\n",
            "Epoch 53/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.9650 - accuracy: 0.5194\n",
            "Epoch 54/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.9124 - accuracy: 0.5359\n",
            "Epoch 55/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 1.8586 - accuracy: 0.5414\n",
            "Epoch 56/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 1.7872 - accuracy: 0.5693\n",
            "Epoch 57/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.7599 - accuracy: 0.5656\n",
            "Epoch 58/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.7110 - accuracy: 0.5807\n",
            "Epoch 59/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.6795 - accuracy: 0.5878\n",
            "Epoch 60/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.6232 - accuracy: 0.6000\n",
            "Epoch 61/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 1.6016 - accuracy: 0.6028\n",
            "Epoch 62/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 1.5531 - accuracy: 0.6130\n",
            "Epoch 63/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 1.5057 - accuracy: 0.6311\n",
            "Epoch 64/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.4691 - accuracy: 0.6348\n",
            "Epoch 65/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 1.4429 - accuracy: 0.6422\n",
            "Epoch 66/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 1.3932 - accuracy: 0.6542\n",
            "Epoch 67/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 1.3759 - accuracy: 0.6597\n",
            "Epoch 68/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.3375 - accuracy: 0.6588\n",
            "Epoch 69/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.3057 - accuracy: 0.6729\n",
            "Epoch 70/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.2785 - accuracy: 0.6767\n",
            "Epoch 71/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.2362 - accuracy: 0.6897\n",
            "Epoch 72/200\n",
            "277/277 [==============================] - 4s 14ms/step - loss: 1.2164 - accuracy: 0.6913\n",
            "Epoch 73/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 1.1806 - accuracy: 0.7045\n",
            "Epoch 74/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.1672 - accuracy: 0.7022\n",
            "Epoch 75/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.1452 - accuracy: 0.7130\n",
            "Epoch 76/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.1284 - accuracy: 0.7172\n",
            "Epoch 77/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.0798 - accuracy: 0.7248\n",
            "Epoch 78/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 1.0769 - accuracy: 0.7237\n",
            "Epoch 79/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 1.0388 - accuracy: 0.7390\n",
            "Epoch 80/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.0249 - accuracy: 0.7425\n",
            "Epoch 81/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 1.0091 - accuracy: 0.7400\n",
            "Epoch 82/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.9873 - accuracy: 0.7466\n",
            "Epoch 83/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.9680 - accuracy: 0.7540\n",
            "Epoch 84/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.9426 - accuracy: 0.7580\n",
            "Epoch 85/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.9225 - accuracy: 0.7676\n",
            "Epoch 86/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.9034 - accuracy: 0.7675\n",
            "Epoch 87/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.8984 - accuracy: 0.7745\n",
            "Epoch 88/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.8822 - accuracy: 0.7712\n",
            "Epoch 89/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.8635 - accuracy: 0.7801\n",
            "Epoch 90/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.8360 - accuracy: 0.7855\n",
            "Epoch 91/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.8169 - accuracy: 0.7899\n",
            "Epoch 92/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.8142 - accuracy: 0.7898\n",
            "Epoch 93/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.7983 - accuracy: 0.7933\n",
            "Epoch 94/200\n",
            "277/277 [==============================] - 3s 12ms/step - loss: 0.7751 - accuracy: 0.8027\n",
            "Epoch 95/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.7822 - accuracy: 0.8001\n",
            "Epoch 96/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.7785 - accuracy: 0.8007\n",
            "Epoch 97/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.7541 - accuracy: 0.8092\n",
            "Epoch 98/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.7425 - accuracy: 0.8106\n",
            "Epoch 99/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.7376 - accuracy: 0.8080\n",
            "Epoch 100/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.7224 - accuracy: 0.8103\n",
            "Epoch 101/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.7085 - accuracy: 0.8179\n",
            "Epoch 102/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6978 - accuracy: 0.8216\n",
            "Epoch 103/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6924 - accuracy: 0.8197\n",
            "Epoch 104/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6746 - accuracy: 0.8257\n",
            "Epoch 105/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.6748 - accuracy: 0.8214\n",
            "Epoch 106/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6534 - accuracy: 0.8289\n",
            "Epoch 107/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6500 - accuracy: 0.8338\n",
            "Epoch 108/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6394 - accuracy: 0.8315\n",
            "Epoch 109/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.6405 - accuracy: 0.8299\n",
            "Epoch 110/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.6448 - accuracy: 0.8301\n",
            "Epoch 111/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.6251 - accuracy: 0.8331\n",
            "Epoch 112/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.6135 - accuracy: 0.8381\n",
            "Epoch 113/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.6134 - accuracy: 0.8385\n",
            "Epoch 114/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.6087 - accuracy: 0.8445\n",
            "Epoch 115/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.6003 - accuracy: 0.8422\n",
            "Epoch 116/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.5870 - accuracy: 0.8457\n",
            "Epoch 117/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5976 - accuracy: 0.8401\n",
            "Epoch 118/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.5724 - accuracy: 0.8472\n",
            "Epoch 119/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.5928 - accuracy: 0.8413\n",
            "Epoch 120/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.5645 - accuracy: 0.8527\n",
            "Epoch 121/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.5567 - accuracy: 0.8513\n",
            "Epoch 122/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5532 - accuracy: 0.8538\n",
            "Epoch 123/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5496 - accuracy: 0.8527\n",
            "Epoch 124/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.5341 - accuracy: 0.8579\n",
            "Epoch 125/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.5379 - accuracy: 0.8578\n",
            "Epoch 126/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.5367 - accuracy: 0.8579\n",
            "Epoch 127/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5246 - accuracy: 0.8603\n",
            "Epoch 128/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5360 - accuracy: 0.8536\n",
            "Epoch 129/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5182 - accuracy: 0.8629\n",
            "Epoch 130/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5280 - accuracy: 0.8550\n",
            "Epoch 131/200\n",
            "277/277 [==============================] - 3s 12ms/step - loss: 0.5173 - accuracy: 0.8611\n",
            "Epoch 132/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5050 - accuracy: 0.8617\n",
            "Epoch 133/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5110 - accuracy: 0.8610\n",
            "Epoch 134/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5129 - accuracy: 0.8624\n",
            "Epoch 135/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.5037 - accuracy: 0.8647\n",
            "Epoch 136/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4957 - accuracy: 0.8648\n",
            "Epoch 137/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.4935 - accuracy: 0.8680\n",
            "Epoch 138/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4844 - accuracy: 0.8678\n",
            "Epoch 139/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4847 - accuracy: 0.8678\n",
            "Epoch 140/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4818 - accuracy: 0.8674\n",
            "Epoch 141/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.4819 - accuracy: 0.8689\n",
            "Epoch 142/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.4739 - accuracy: 0.8702\n",
            "Epoch 143/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4798 - accuracy: 0.8703\n",
            "Epoch 144/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4729 - accuracy: 0.8725\n",
            "Epoch 145/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4739 - accuracy: 0.8732\n",
            "Epoch 146/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4719 - accuracy: 0.8730\n",
            "Epoch 147/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.4671 - accuracy: 0.8707\n",
            "Epoch 148/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.4652 - accuracy: 0.8710\n",
            "Epoch 149/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4717 - accuracy: 0.8712\n",
            "Epoch 150/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4617 - accuracy: 0.8721\n",
            "Epoch 151/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4601 - accuracy: 0.8727\n",
            "Epoch 152/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4599 - accuracy: 0.8716\n",
            "Epoch 153/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4553 - accuracy: 0.8770\n",
            "Epoch 154/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4498 - accuracy: 0.8732\n",
            "Epoch 155/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4558 - accuracy: 0.8741\n",
            "Epoch 156/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4373 - accuracy: 0.8786\n",
            "Epoch 157/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4425 - accuracy: 0.8765\n",
            "Epoch 158/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.4412 - accuracy: 0.8755\n",
            "Epoch 159/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4480 - accuracy: 0.8754\n",
            "Epoch 160/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4469 - accuracy: 0.8756\n",
            "Epoch 161/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4313 - accuracy: 0.8814\n",
            "Epoch 162/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4363 - accuracy: 0.8773\n",
            "Epoch 163/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.4374 - accuracy: 0.8787\n",
            "Epoch 164/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.4366 - accuracy: 0.8780\n",
            "Epoch 165/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4262 - accuracy: 0.8805\n",
            "Epoch 166/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4318 - accuracy: 0.8756\n",
            "Epoch 167/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4200 - accuracy: 0.8823\n",
            "Epoch 168/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4273 - accuracy: 0.8804\n",
            "Epoch 169/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4208 - accuracy: 0.8799\n",
            "Epoch 170/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4245 - accuracy: 0.8778\n",
            "Epoch 171/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4201 - accuracy: 0.8806\n",
            "Epoch 172/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4198 - accuracy: 0.8813\n",
            "Epoch 173/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4224 - accuracy: 0.8806\n",
            "Epoch 174/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.4125 - accuracy: 0.8847\n",
            "Epoch 175/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4198 - accuracy: 0.8817\n",
            "Epoch 176/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4128 - accuracy: 0.8842\n",
            "Epoch 177/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4239 - accuracy: 0.8754\n",
            "Epoch 178/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4017 - accuracy: 0.8894\n",
            "Epoch 179/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4057 - accuracy: 0.8828\n",
            "Epoch 180/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.4028 - accuracy: 0.8847\n",
            "Epoch 181/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4113 - accuracy: 0.8846\n",
            "Epoch 182/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4020 - accuracy: 0.8856\n",
            "Epoch 183/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4104 - accuracy: 0.8828\n",
            "Epoch 184/200\n",
            "277/277 [==============================] - 2s 9ms/step - loss: 0.4075 - accuracy: 0.8831\n",
            "Epoch 185/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.4035 - accuracy: 0.8848\n",
            "Epoch 186/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3958 - accuracy: 0.8876\n",
            "Epoch 187/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4052 - accuracy: 0.8864\n",
            "Epoch 188/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3943 - accuracy: 0.8881\n",
            "Epoch 189/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3999 - accuracy: 0.8803\n",
            "Epoch 190/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.3981 - accuracy: 0.8856\n",
            "Epoch 191/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3988 - accuracy: 0.8861\n",
            "Epoch 192/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.4032 - accuracy: 0.8887\n",
            "Epoch 193/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3904 - accuracy: 0.8843\n",
            "Epoch 194/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3845 - accuracy: 0.8883\n",
            "Epoch 195/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.3833 - accuracy: 0.8916\n",
            "Epoch 196/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.3909 - accuracy: 0.8849\n",
            "Epoch 197/200\n",
            "277/277 [==============================] - 2s 8ms/step - loss: 0.3976 - accuracy: 0.8846\n",
            "Epoch 198/200\n",
            "277/277 [==============================] - 3s 11ms/step - loss: 0.3807 - accuracy: 0.8895\n",
            "Epoch 199/200\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.3866 - accuracy: 0.8876\n",
            "Epoch 200/200\n",
            "277/277 [==============================] - 3s 10ms/step - loss: 0.3899 - accuracy: 0.8878\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assuming you have X, y, total_words defined\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words + 1, 100, input_length=X.shape[1]))\n",
        "model.add(LSTM(units=150, return_sequences=True, kernel_initializer=\"random_uniform\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=150, return_sequences=False, kernel_initializer=\"random_uniform\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=total_words + 1, activation=\"softmax\", kernel_initializer=\"random_uniform\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_one_hot = to_categorical(y, num_classes=total_words + 1)\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on the entire dataset with early stopping\n",
        "history = model.fit(X, y_one_hot, epochs=200, callbacks=[early_stopping])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving the model"
      ],
      "metadata": {
        "id": "h3qxZWgUGgWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('nextwordDLQ&A.h5')"
      ],
      "metadata": {
        "id": "kaibjk7IGgi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving the tokenizer"
      ],
      "metadata": {
        "id": "v0u2RRRWKvqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'questions' is your list of strings\n",
        "tk.fit_on_texts(questions)\n",
        "\n",
        "# Save the tokenizer using pickle\n",
        "with open('/content/tokenizer.pkl', 'wb') as tk_file:\n",
        "    pickle.dump(tk, tk_file)\n",
        "\n",
        "print(\"Tokenizer saved successfully.\")\n"
      ],
      "metadata": {
        "id": "5Y70Yr_eK2KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the tokenizer"
      ],
      "metadata": {
        "id": "MBpns3MkLFlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the tokenizer from the saved pickle file\n",
        "with open('/content/drive/MyDrive/tokenizer.pkl', 'rb') as tk_file:\n",
        "    loaded_tokenizer = pickle.load(tk_file)\n",
        "\n",
        "# Now, 'loaded_tokenizer' can be used for text processing\n"
      ],
      "metadata": {
        "id": "ObHOHU7bK22U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "ztV4AK2SDIBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/next word rnn lstm/nextwordDLQ&A.h5')\n",
        "\n",
        "def predict_next_word(seed_text, model, tokenizer, max_sequence_length):\n",
        "    for _ in range(10):  # Adjust the number of predictions as needed\n",
        "        # Tokenize the seed text\n",
        "        token_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        if not token_text:\n",
        "            print(\"Empty token_text. Ensure your tokenizer is trained correctly.\")\n",
        "            break\n",
        "\n",
        "        # Padding\n",
        "        padded_token_text = pad_sequences([token_text], maxlen=max_sequence_length, padding='pre')\n",
        "        # Predict\n",
        "        predictions = model.predict(padded_token_text)\n",
        "        # Get the index of the predicted word\n",
        "        pos = np.argmax(predictions)\n",
        "\n",
        "        # Map index back to word using the tokenizer\n",
        "        word = tokenizer.index_word.get(pos, None)\n",
        "        if word:\n",
        "            # Update the seed text\n",
        "            seed_text += \" \" + word\n",
        "            print(seed_text)\n",
        "        else:\n",
        "            print(\"Predicted word not found in the vocabulary.\")\n",
        "            break\n",
        "\n",
        "# Assuming max_sequence_length is the length used during training\n",
        "max_sequence_length = 27\n",
        "predict_next_word(\"this is\", model, tk, max_sequence_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1doMiDdKF_O",
        "outputId": "350a4659-24e1-48bb-deb8-7b593ace36fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "this is a\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "this is a pretty\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "this is a pretty intuitive\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "this is a pretty intuitive answer\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "this is a pretty intuitive answer as\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "this is a pretty intuitive answer as we\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "this is a pretty intuitive answer as we saw\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "this is a pretty intuitive answer as we saw above\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "this is a pretty intuitive answer as we saw above we\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "this is a pretty intuitive answer as we saw above we perform\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model is giving the prediction words"
      ],
      "metadata": {
        "id": "kaYkuRq5L-eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "okprX6EqHCvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1f3581-539c-4ec2-9895-3e1aa088bea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7dmm-bhjoiV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6500ba63-ac9a-4b76-9816-873bed00d846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 27, 100)           177500    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 27, 150)           150600    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 27, 150)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1775)              268025    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 776725 (2.96 MB)\n",
            "Trainable params: 776725 (2.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuwvsnBBpSGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}